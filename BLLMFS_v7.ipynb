{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd9587c-633a-46e5-ad11-7da331993228",
   "metadata": {},
   "source": [
    "# 1 Understanding Large Language Models\n",
    "## 1.1 definitions\n",
    "![fig1-1](px/fig1-1.png)\n",
    "## 1.2 applications\n",
    "![fig1-2](px/fig1-2.png)\n",
    "## 1.3 build stages\n",
    "![fig1-3](px/fig1-3.png)\n",
    "- custom builds are good\n",
    "- pretraining (base model, ex: GPT-3) & fine-tuning (on labeled data)\n",
    "\t- _instruction_ finetuning vs _classification_ finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f6718-d828-4b6a-9d26-ea15c83856f1",
   "metadata": {},
   "source": [
    "## 1.4 transformers\n",
    "![fig1-4](px/fig1-4.png)\n",
    "\n",
    "- \"attention is all you need\": [https://arxiv.org/abs/1706.03762]\n",
    "- encoder (lang -> vect), decoder (vect -> lang)\n",
    "- self-attention: weightings of tokens in sequence relative positioning\n",
    "\n",
    "![fig1-5](px/fig1-5.png)\n",
    "- BERT = transformer variant (bidirectional)\n",
    "\t- specializes in __masked word prediction__.\n",
    "\t- X (twitter) uses BERT to see toxic content.\n",
    "- GPTx = transformer variant\n",
    "\t- focuses on decoder --> designed for _generating_ texts.\n",
    " \n",
    "![fig1-6](px/fig1-6.png)\n",
    "- BERT and GPT are both good at __zero_shot__ & __few_shot__ learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ae10c-d659-408c-a4d4-4ecacf473de3",
   "metadata": {},
   "source": [
    "## 1.5 datasets\n",
    "GPT3 training data:\n",
    "\n",
    "![table1-1](px/table1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfee605-f73c-48d9-9aa5-7d1d2d158c7a",
   "metadata": {},
   "source": [
    "## 1.6 GPT architecture\n",
    "- GPT stands for __Generative Pretrained Transformer__.\n",
    "- “Improving Language Understanding by Generative Pre-Training” [https://mng.bz/x2qg] by Radford et al. from OpenAI.\n",
    "- GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset.\n",
    "- The original ChatGPT model was created by finetuning GPT-3 on a large instruction dataset using a method from __OpenAI’s InstructGPT__ paper (see chapter 7).\n",
    "- Next-word prediction is a form of __self-supervised learning__ (aka self-labeling). We don’t need to collect labels for the training data explicitly but can leverage the\n",
    "structure of the data itself: we can use the next word in a sentence or document as the label that the\n",
    "model is supposed to predict.\n",
    "- Since this allows us to create labels “on the fly,” it is possible to leverage massive unlabeled text datasets to train LLMs.\n",
    "\n",
    "![fig1-8](px/fig1-8.png)\n",
    "\n",
    "- The general GPT architecture is relatively simple. It’s just the decoder part without the encoder.\n",
    "- Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of __autoregressive model__. Autoregressive models\n",
    "incorporate their previous outputs as inputs for future predictions. \n",
    "- Architectures such as GPT-3 are significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56952a-b9c5-4289-a8cd-1ca865026bce",
   "metadata": {},
   "source": [
    "## 1.7 building\n",
    "![fig1-9](px/fig1-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2752c5-5581-4e74-999e-5535a1d284a9",
   "metadata": {},
   "source": [
    "# 2 Working with Text Data\n",
    "## 2.1 word embeddings\n",
    "- continuous-valued vectors\n",
    "- word level most common. sentence-, paragraph-, document-level embeds also used.\n",
    "- sentence- & paragraph-level embeds = popular for RAG.\n",
    "- RAG: combines text generation with retrieval (searching extern knowledge base)\n",
    "- word2vec\n",
    "- LLMs commonly build their own embeddings\n",
    "- dimensionality: smallest GPT2 = 768 dimensions; largest GPT-3 = 12288 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b65147-7149-4ee6-a62e-e3215da4d32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#characters: 20480 \n",
      "\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "\traw_text = f.read()\n",
    "\n",
    "print(\"#characters:\", len(raw_text), \"\\n\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264410e-a9ce-4437-9ddd-a4ac76d49462",
   "metadata": {},
   "source": [
    "## 2.2 tokenizing\n",
    "![fig2-4](px/fig2-4.png)\n",
    "- the-verdict.txt (https://en.wikisource.org/wiki/The_Verdict)\n",
    "- whitespace removal: depends on application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa582e4-2366-444e-a7a0-9d0a9a5ac27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' ', 'world,', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
      "['hello', ' ', 'world', ',', '', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['hello', 'world', ',', 'this', 'is', 'a', 'test', '.']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "import re \n",
    "t = \"hello world, this is a test.\"\n",
    "print(re.split(r'(\\s)',t))\n",
    "\n",
    "# tokenization - splits on whitespaces, commas, periods\n",
    "print(re.split(r'([,.]|\\s)',t))\n",
    "\n",
    "# remove redundant whitespaces\n",
    "print([item for item in re.split(r'([,.]|\\s)',t) if item.strip()])\n",
    "\n",
    "# also handle \"?\", quote marks, double-dashes\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165c283-0a28-49aa-98f9-47cdc69f96f5",
   "metadata": {},
   "source": [
    "## 2.3 tokens to IDs\n",
    "- We previously tokenized a short story by Edith Wharton into individual tokens. Now we will convert these tokens from a Python string to an integer representation to produce __token IDs__. This is an intermediate step before converting token IDs into embedding vectors.\n",
    "- To map tokens into token IDs, we have to build a vocabulary. It defines how we map each unique word and special character to a unique integer.\n",
    "![fig2-6](px/fig2-6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a040efc8-ac76-4d0c-978d-35eb8b4e3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "# apply tokenizer to short story - answer should be 4649 tokens.\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ffa1c25-f568-427a-9081-a4f08d345e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "# build list of unique tokens, sort by alpha. should be 1159 total.\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c470005-4652-44d4-8be6-8f599d0076c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n"
     ]
    }
   ],
   "source": [
    "# print first 51 entries \n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "\tprint(item)\n",
    "\tif i >= 50:\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b8e69-b6d9-4936-adbe-079b7ff1bd0b",
   "metadata": {},
   "source": [
    "![fig2-8](px/fig2-8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81f9dff-9cdf-4990-befb-45b2bd34c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "\n",
    "\tdef __init__(self, vocab):\n",
    "\t\t# store vocab as class attribute\n",
    "\t\tself.str_to_int = vocab \n",
    "\t\t# create inverse vocab: maps tokens back to orig text tokens\n",
    "\t\tself.int_to_str = {i:s for s,i in vocab.items()}#B\n",
    "\n",
    "\t# converts text --> token IDs\n",
    "\tdef encode(self, text):\n",
    "\t\tpreprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "\t\tpreprocessed = [\n",
    "\t\t\titem.strip() for item in preprocessed if item.strip()\n",
    "\t\t]\n",
    "\t\tids = [self.str_to_int[s] for s in preprocessed]\n",
    "\t\treturn ids\n",
    "\t\n",
    "\t# converts token IDs --> text\n",
    "\tdef decode(self, ids):\n",
    "\t\ttext = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\t\t# replaces spaces before specified punctuation\n",
    "\t\ttext = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\t\treturn text\n",
    "\n",
    "# tokenize a passage from the short story:\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "# turn these tokens back into text:\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3c1a26-ba35-43dd-a731-dfb8674e29d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry, but a word from this query is not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# apply to new sample, not in training set:\n",
    "# (\"Hello\" not in vocabulary - bounces out with an error.)\n",
    "try:\n",
    "    print(tokenizer.encode(\"Hello, do you like tea?\"))\n",
    "except Exception:\n",
    "    print(\"sorry, but a word from this query is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f72645-b0e0-4eb9-b974-48ac69b59ef1",
   "metadata": {},
   "source": [
    "## 2.4 special context tokens\n",
    "![fig2-9](px/fig2-9.png)\n",
    "- mods tokenizer to use a \"junk\" token for unknown words\n",
    "- mods tokenizer to add a token between unrelated texts (aka, between books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7ed82c-e790-4417-b206-30baab1c3ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "# modify vocab to include \"junk\" and \"endoftext\" tokens. new vocab size should be 1161.\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n",
    "\n",
    "# last 5 entries of updated vocab:\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "\tprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb657372-a557-4b77-a4b0-06c3623456ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "\tdef __init__(self, vocab):\n",
    "\t\tself.str_to_int = vocab\n",
    "\t\tself.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "\tdef encode(self, text):\n",
    "\t\tpreprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "\t\tpreprocessed = [\n",
    "\t\t\titem.strip() for item in preprocessed if item.strip()\n",
    "\t\t]\n",
    "\t\t# replaces unknown words with junk token\n",
    "\t\tpreprocessed = [item if item in self.str_to_int\n",
    "\t\t\t\t\t\telse \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "\t\tids = [self.str_to_int[s] for s in preprocessed]\n",
    "\t\treturn ids\n",
    "\n",
    "\tdef decode(self, ids):\n",
    "\t\ttext = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\t\t# replaces spaces before specified punctuation\n",
    "\t\ttext = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\t\treturn raw_text\n",
    "\n",
    "# test using two text samples, concat'd with two unrelated sources\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text))[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a1f07-d5c9-46b0-b94b-2ddef71de49d",
   "metadata": {},
   "source": [
    "## 2.5 byte pair encoding (bpe)\n",
    "- pip install tiktoken ([https://github.com/openai/tiktoken])\n",
    "- instantiate BPE tokenizer using GPT-2 encoding\n",
    "- endoftext token assigned to large id#\n",
    "- BPE tokenizer handles unknown words w/o errors\n",
    "\t- breaks unknowns into subwords or characters\n",
    "\t- BPE builds vocab by merging frequent chars into subwords & frequent subwords into words.\n",
    "\t- merge frequency handled by a cutoff variable.\n",
    "\n",
    "![fig2-11](px/fig2-11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08d29a2-b49d-4e7e-8a40-f99c5efe629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken vers# 0.7.0\n",
      "encoded integers:\n",
      " [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "decoded tokens:\n",
      " Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print (\"tiktoken vers#\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(\"encoded integers:\\n\",integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(\"decoded tokens:\\n\",strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338fc77-14dc-4f72-b22b-a3c92d8ff892",
   "metadata": {},
   "source": [
    "## 2.6 data sampling - sliding window\n",
    "- build a data loader - fetches input-target pairs\n",
    "- tokenize 'the verdict' using BPE\n",
    "- build shifter\n",
    "- build loader that returns inputs & targets as Pytorch tensors\n",
    "- listing 2.5 --> listing26.py\n",
    "- listing 2.6 --> listing26.py\n",
    "- test with batch size = 1\n",
    "\t- returns tensor (input token IDs) and tensor (target token IDs)\n",
    "\t- max length = 4, so tensors contain 4 values each.\n",
    "\t- common to train LLMs with input sizes >= 256.\n",
    "\t- small batch sizes: less memory needed, but more noisy.\n",
    "- repeat test with batch size > 1\n",
    "\n",
    "![fig2-12](px/fig2-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0c791c-6a72-409c-b98e-2f9aaac1b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n",
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "\traw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# x,y = inputs shifted by one position.\n",
    "context_size = 4 #A\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "# create next-word prediction task\n",
    "for i in range(1, context_size+1):\n",
    "\tcontext = enc_sample[:i]\n",
    "\tdesired = enc_sample[i]\n",
    "\tprint(context, \"---->\", desired)\n",
    "\n",
    "# repeat prediction with token IDs converted to text\n",
    "for i in range(1, context_size+1):\n",
    "\tcontext = enc_sample[:i]\n",
    "\tdesired = enc_sample[i]\n",
    "\tprint(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255097d3-9c38-46e1-b21a-306590efdb6a",
   "metadata": {},
   "source": [
    "- Now we need a data loader.\n",
    "\n",
    "![fig2-13](px/fig2-13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6630353e-e7b9-4b43-be77-3360f6b96d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch data loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "\tdef __init__(self, txt, tokenizer, max_length, stride):\n",
    "\t\tself.input_ids = []\n",
    "\t\tself.target_ids = []\n",
    "\t\ttoken_ids = tokenizer.encode(txt)\n",
    "\n",
    "\t\t# use sliding window to chunk book into overlapping sequences of max_length\n",
    "\t\tfor i in range(0, len(token_ids) - max_length, stride):\n",
    "\t\t\tinput_chunk  = token_ids[i:     i + max_length    ]\n",
    "\t\t\ttarget_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            \n",
    "\t\t\tself.input_ids.append(torch.tensor(input_chunk))\n",
    "\t\t\tself.target_ids.append(torch.tensor(target_chunk))\n",
    "\t\n",
    "\t# return total #rows in dataset\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.input_ids)\n",
    "\t\n",
    "\t# return single row from dataset\n",
    "\tdef __getitem__(self, idx): #D\n",
    "\t\treturn self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\ttokenizer = tiktoken.get_encoding(\"gpt2\") #init\n",
    "\tdataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # create dataset\n",
    "\tdataloader = DataLoader(\n",
    "\t\tdataset,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=shuffle,\n",
    "\t\tdrop_last=drop_last, # drops last batch if shorter than batch_size\n",
    "\t\tnum_workers=0 # number of CPU processes\n",
    "\t\t)\n",
    "\treturn dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8a7925a-7dda-4721-bf17-08a73e034935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch:\n",
      " [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# test with batch size = 1, LLM context size = 4\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "\traw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "\traw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader) # convert to python iterator (uses python next function)\n",
    "first_batch = next(data_iter)\n",
    "print(\"first batch:\\n\",first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2db393-df81-49ef-850e-1a1058d038eb",
   "metadata": {},
   "source": [
    "![fig2-14](px/fig2-14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2de4d20-9c52-4504-8d61-ce6f14561bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# 2nd batch output = shifted by one position\n",
    "# 'stride' sets number of positions to shift across batches.\n",
    "# repeat with batch size > 1\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3202b7-1dba-473c-acb0-43f58d5ef0f8",
   "metadata": {},
   "source": [
    "## 2.7 token embeddings\n",
    "- note: embeddings are init'd with random values.\n",
    "- embed layer has 6 rows (one per possible vocab token)\n",
    "- embed layer has 3 cols (one per embed dimension)\n",
    "- embed layers = better way of doing one-hot encoding followed by mat.mul. in a fully connected layer. [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb]\n",
    "\n",
    "![fig2-15](px/fig2-15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66697b21-2c80-4ec2-9c47-b05f1e0e2ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding layer weights:\n",
      " Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "embeding layer weights, 4th row\n",
      " tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# token IDs -> embedding vector conversion - simple illustration\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# create pytorch embed layer\n",
    "# use random seed of 123 for reproducability\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(\"embedding layer weights:\\n\",embedding_layer.weight)\n",
    "\n",
    "# apply to a token ID - returns embed vector - should match 4th row of embed layer.\n",
    "print(\"embeding layer weights, 4th row\\n\",embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eb232-5aa6-444d-9136-75cc89544f2e",
   "metadata": {},
   "source": [
    "- EMBEDDING LAYERS VERSUS MATRIX MULTIPLICATION\n",
    "- The embedding layer approach described here is a more efficient way of doing __one-hot encoding__ followed by matrix multiplication in a __fully connected layer__ (see the GitHub at [https://mng.bz/ZEB5]). It can be seen as a neural network layer that can be optimized via backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fb0ba-847b-4e79-b693-15a6059d0398",
   "metadata": {},
   "source": [
    "## 2.8 encoding word positions\n",
    "- self-attention mech doesn't know how to handle positions or order.\n",
    "- position-aware embed types:\n",
    "\t- __relative__ position (aka distance btwn tokens)\n",
    "\t- __absolute__ position (specific positions in a sequence)\n",
    "- GPT uses absolute position embedding, optimized during training.\n",
    "- context_length = LLM's supportable input size.\n",
    "\n",
    "![fig2-17](px/fig2-17.png)\n",
    "\n",
    "![fig2-18](px/fig2-18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d021ec-4d50-48a9-8cb8-8dc759bb8981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# new example: use 256 for output dim & 50257 (BPE tokenizer) for vocab size\n",
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# sample from dataloader - embed each token in each batch. \n",
    "# batch size = 8, 4 tokens each --> 8x4x256 result.\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "\traw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4e3b5d0-54af-40f9-bc2b-dedac56f5059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# use embed layer to embed token IDs to 256-d vector.\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# for GPT absolute encoding approach - create another embed layer with same dims\n",
    "context_length      = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings      = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# add positional embeddings to token embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefed51-99a6-4d0c-85ec-b8fd2fd6a465",
   "metadata": {},
   "source": [
    "# 3 (Self) Attention Mechanisms\n",
    "- four variants to discuss:\n",
    "\t- simplified SA\n",
    "\t- SA\n",
    "\t- \"casual\" attention (considers only prev & crnt inputs in a sequence)\n",
    "\t- multihead attention (enables using info from multi representations)\n",
    "\n",
    "![fig3-2](px/fig3-2.png)\n",
    "\n",
    "## 3.1 long sequences\n",
    "- need context for translation. word-for-word matching not doable.\n",
    "- before transformers, RNNs were most popular encode-decode architecture for translations.\n",
    "- encoders & decoders use _hidden states_ (memory cell) to capture context info.\n",
    "- enc/dec RNN limitation: it relies solely on __entire__ current hidden state being available before passing to a decoder\n",
    "\n",
    "## 3.2 capturing data dependencies with attention mechanism\n",
    "\n",
    "Before transformer LLMs, it was common to use RNNs for language translation. RNNs work fine for translating short sentences but not for longer texts - they don’t have direct access to previous words in the input.\n",
    "\n",
    "RNNs must remember the entire encoded input in a single hidden state before passing it to the decoder.\n",
    "\n",
    "The Bahdanau attention mechanism modifies the encoder-decoder RNN - the decoder can select varying parts of the input sequence at each decoding step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a3b09-fee0-4020-8762-66cd85f51b57",
   "metadata": {},
   "source": [
    "## 3.3 self-attention\n",
    "- \"self\": ability to get weights from different positions in a single input sequence.\n",
    "- traditional attention: focused on relations between elements of two sequences.\n",
    "\n",
    "### 3.3.1 self attention - no trainable weights\n",
    "- \"x\" input sequence of \"T\" elements - text already converted to token embeddings.\n",
    "- each element corresponds to \"d\"-dimensioned vector for specific token.\n",
    "- goal: find context vectors z_i for each element x_i in input sequence.\n",
    "- more reading: dot products\n",
    "- normalize to get a total weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cddf500e-79b2-4520-9c21-69b99204a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# sample sequence, already converted to 3D vectors. (small to prevent line breaks.)\n",
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your\n",
    "[0.55, 0.87, 0.66], # journey\n",
    "[0.57, 0.85, 0.64], # starts\n",
    "[0.22, 0.58, 0.33], # with\n",
    "[0.77, 0.25, 0.10], # one\n",
    "[0.05, 0.80, 0.55]] # step\n",
    ")\n",
    "\n",
    "# find \"w\" attention scores dot product of query (x_2) & every other input token.\n",
    "# 2nd token serves as the query.\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "\tattn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d3cb1-8fdd-4e10-a425-6e0b65a9f805",
   "metadata": {},
   "source": [
    "- The goal is to obtain attention weights that sum up to 1. This convention helps interpretation and maintaining training stability in an LLM. Here’s a straightforward method for achieving this normalization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59d2ad9c-ab15-4163-a642-22763cb33c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9499d9-9f87-4ede-938a-381854baa905",
   "metadata": {},
   "source": [
    "- In practice, it’s more common and advisable to use the softmax function for normalization. It's better at managing extreme values and has better gradient properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b82abff-adf5-4b5e-ae70-eb455c4b6fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd98b07-e521-4cff-b844-f1fc05a1dbb3",
   "metadata": {},
   "source": [
    "- Softmax also ensures __attention weights are always positive__. This makes the output interpretable as probabilities or relative importance (higher weights = greater importance).\n",
    "- This naive softmax implementation may encounter overflow and underflow problems when dealing with large or small input values. __Use the PyTorch implementation__ of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c0abdb4-4259-47b8-bda0-e71663c51b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f31f4-739c-489b-a309-59d969afac8d",
   "metadata": {},
   "source": [
    "- Calculating the __context vector__ z(2) by multiplying the __embedded input tokens__, x(i), with the corresponding __attention weights__ and then summing the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb6a8eea-e206-4999-ba26-74a4a53b166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae559af1-e1b6-44f9-86df-ed6f85de9e91",
   "metadata": {},
   "source": [
    "### 3.3.2 - attention weights for all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "862211ec-11c1-427f-a5fc-2171c7ebacd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25fcdf-72e2-4198-bc77-597baaecd7cb",
   "metadata": {},
   "source": [
    "- Each element in the tensor is __an attention score between each pair of inputs__. We previously for-loops in Python. They are generally slow - get the same results using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02a8210e-2076-40de-80e7-98d748c13637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ca56e74-4a16-453e-aea2-a596a4677074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# normalize each row\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c77d34-bb4f-47b2-9fe4-3fa0583da6f4",
   "metadata": {},
   "source": [
    "- The 'dim' parameter specifies the dimension of the input tensor along which the function will be computed. __dim=-1__ instructs softmax to apply the normalization along the __last dimension__ of the attn_scores tensor. If attn_scores is a 2D tensor (for example, with a shape of [rows,columns]), dim=-1 will normalize across the columns so that the values in each row (summing over the column dimension) sum up to 1.\n",
    "- Verify rows all sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b384d7a1-7e18-40fc-9aa4-4ea5995cf233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e7718-642b-4599-a604-481ceb02dc47",
   "metadata": {},
   "source": [
    "- Use attention weights to compute all context vectors via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47bcc3d9-906a-4d7f-b737-da7361e244d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14340626-6321-424b-a1c7-6e126285b564",
   "metadata": {},
   "source": [
    "- Verify code is correct by comparing 2nd row with the context vector z(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fac207c-faee-471c-8c30-f604e1c84371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38763c6-d868-4fcb-ab57-0e4fa12c0985",
   "metadata": {},
   "source": [
    "## 3.4 - attention with trainable weights\n",
    "- also called _scaled dot-product attention_.\n",
    "  self-attention mechanism with trainable weights builds on its predecessor: it __builds context vectors__ as weighted sums over the input vectors specific to a certain input element.\n",
    "- It introduces __weight matrices__ that are updated during training. They enable the model (specifically the attention module) to learn how to produce “good” context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8cc81-66c6-4e02-bb09-74c9e5d1e5e8",
   "metadata": {},
   "source": [
    "### 3.4.1 - step by step\n",
    "- We will define __three trainable weight matrices__ Wq, Wk, and Wv. They are used to project the embedded input tokens, x(i), into query, key, and value vectors,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c95a828-6d47-4b8e-a4e3-d1fa91b0023a",
   "metadata": {},
   "source": [
    "- We earlier defined the second input element x(2) as the query when we computed simplified attention weights to find the context vector z(2).\n",
    "- Later we generalized this to find all context vectors z(1) ... z(T) for the six-word input sentence “Your journey starts with one step.”\n",
    "- Now, start by computing only one context vector, z(2), for illustration. Next we will modify the code to calculate all context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f21690-79f0-4493-9f1e-509b7efbc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #2nd input element\n",
    "d_in = inputs.shape[1] #input embed size\n",
    "d_out = 2 # output embed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51ea0a3f-3fbe-405c-b9f8-799563e079f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Wq, Wk, Wv\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# requires_grad=False reduces clutter in the outputs. if using weight matrices for model training, set requires_grad=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1cb844b-f61c-4ef9-94ad-11d582db3e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf130b2-4566-45d3-8194-0d664a70ec08",
   "metadata": {},
   "source": [
    "- Even though this goal is only to find one context vector, z(2), we still require the key and value vectors for all input elements. They are involved in computing the attention weights w.r.t. the query q(2). We can obtain all keys and values via matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d07d5c12-7fc3-4877-b0bc-c76bb4470e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# projects 6 input tokens from a 3D space into a 2D embedded space\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52421d73-125d-45f0-9ac8-9a9b10bf5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# find attention score w_22\n",
    "keys_2        = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c61f2b1c-f2a5-4523-ab55-adf3390b4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# generalize this step to all attention scores for a given query\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1398819-bec2-45e4-81a9-01ef63f25e81",
   "metadata": {},
   "source": [
    "- Third step: finding the attention weights by scaling the attention scores and using the softmax function. This time, __scale the attention scores__ by dividing them by the square root of the embedding dimension of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a7f24cc-64c9-4bcf-87e9-3f5a943b05bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0cc30-bd9f-463d-8f5b-07a8781b7bd3",
   "metadata": {},
   "source": [
    "#### SCALED-DOT PRODUCT ATTENTION\n",
    "\n",
    "- normalization by embedding dimension size __improves training performance by avoiding small gradients__. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in __very small gradients during backpropagation__ due to softmax. As dot products increase, __softmax behaves more like a step function__ with gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31b500-40aa-4e95-bb96-96884417309a",
   "metadata": {},
   "source": [
    "- Compute the context vector as a weighted sum over the value vectors. Here, the __attention weights serve as a weighting factor__ for the respective importance of each value\n",
    "vector. We can use matrix multiplication to obtain the output in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f0037d4-67d4-4666-99db-39b37c2b0159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcbdd8-e61e-4fb5-b84d-cea34617007e",
   "metadata": {},
   "source": [
    "### 3.4.2 building a compact self-attention Python class\n",
    "- __SelfAttention_v1__ initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension __d_out__.\n",
    "- The forward method computes attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
    "- Finally, we create a context vector by weighting the values with these normalized attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c2ebec3-3e08-4a36-bb74-572f00eae071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys         = x @ self.W_key\n",
    "        queries      = x @ self.W_query\n",
    "        values       = x @ self.W_value\n",
    "        attn_scores  = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec  = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eec8a15-badd-44e8-b770-9e36c1278809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Since inputs contains six embedding vectors, this results in a matrix storing the six context vectors:\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d1f81de-9665-4c6d-981d-8d8af3fe4b1a",
   "metadata": {},
   "source": [
    "![figure 3-18](px/fig3-18.png)\n",
    "- Self-attention transforms input vectors (X) with three weight matrices (Wq, Wk, and Wv).\n",
    "- Find the __attention weight matrix__ using the resulting queries (Q) and keys (K).\n",
    "- Using the __attention weights and values__ (V), find the __context vectors__ (Z).\n",
    "- The 3D input tensor is simplified to a two-dimensional matrix in this context.\n",
    "- This approach allows for a more straightforward visualization and understanding of the processes involved. Also, for\n",
    "consistency with later figures, the values in the attention matrix do not depict the real attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dfdf0-5598-4078-8fcf-776acac3a496",
   "metadata": {},
   "source": [
    "- We can __improve SelfAttention_v1__ by using PyTorch’s __nn.Linear__ layers - they can do matrix multiplication when the bias units are disabled.\n",
    "- Also, nn.Linear has an __optimized weight initialization__ scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef3adaf0-c46b-4587-a833-8349f769b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys         = self.W_key(x)\n",
    "        queries      = self.W_query(x)\n",
    "        values       = self.W_value(x)\n",
    "        attn_scores  = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec  = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511506b3-fd5f-4cdd-ab57-56adbba2449f",
   "metadata": {},
   "source": [
    "- SelfAttention_v1 & SelfAttention_v2 give different outputs - they use __different initial weights for the weight matrices__, since nn.Linear uses a more sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94ea1d69-7e90-48e1-bcce-2141e2aba883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f418e76-c074-44d3-acbe-4a37c52432f4",
   "metadata": {},
   "source": [
    "## 3.5 - hiding future words with causal attention\n",
    "\n",
    "- Causal attention, aka __masked attention__, restricts a model to consider only __previous and current inputs__ in a sequence when processing any given token. (standard self-attention mechanism allows access to the entire input sequence at once.)\n",
    "- Causal attention mechanism ensures that the model only factors in tokens that occur at or before the current token in the sequence. It masks future tokens, which\n",
    "come after the current token in the input text.\n",
    "- It masks attention weights above the diagonal & normalizes the nonmasked attention weights so __the attention weights sum to 1 in each row__.\n",
    "\n",
    "![fig3-19](px/fig3-19.png)\n",
    "\n",
    "- Step 1: compute the attention weights using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9d939e1-a589-4111-a386-72b7da1d85cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries      = sa_v2.W_query(inputs) #A\n",
    "keys         = sa_v2.W_key(inputs)\n",
    "attn_scores  = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c19a17-918f-43bd-ac09-d8cdd0d804b8",
   "metadata": {},
   "source": [
    "- Step 2: use __PyTorch’s tril function__ to create a mask where the values above the diagonal are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f502c80d-5cc8-4c75-9fbd-4c982f4f24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple   = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9e309-da1b-4f93-a6a9-fd5b9bbe0c06",
   "metadata": {},
   "source": [
    "- Multiply the weights with the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d6275de-d551-47b2-bb0c-a7e96cd1fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421521cb-b203-4714-b4ae-3c7da3023e26",
   "metadata": {},
   "source": [
    "- Step 3: renormalize the attention weights to sum up to 1 in each row. We can achieve this by dividing each element in each row by the sum in each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69240e34-5fec-4a35-aa8e-4e97357ff5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums           = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a689d-7a5d-4fd7-a7eb-0a755fc960e9",
   "metadata": {},
   "source": [
    "### INFORMATION LEAKAGE\n",
    "- When we apply a mask and then renormalize the attention weights, it appears that information from future tokens (which we intend to mask) could still influence the\n",
    "current token because their values are part of the softmax calculation.\n",
    "- When we renormalize the attention weights after masking, we’re effectively recalculating the softmax over a smaller subset (since masked positions don’t contribute to the softmax value).\n",
    "- The elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified.\n",
    "- In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there’s no information leakage from future (or otherwise masked) tokens as we intended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c14ed-c421-4b7b-ac80-8a23f06199ac",
   "metadata": {},
   "source": [
    "- Take advantage of a property of softmax & implement masked attention weightings more efficiently in fewer steps.\n",
    "- Softmax converts its inputs into a probability distribution. When negative infinity values (-∞) are present in a row, softmax treats them as zero probability.\n",
    "(Mathematically, this is because e∞ approaches 0.) We can implement this more efficient masking “trick” by creating a mask with 1s above the diagonal and then replacing these 1s with negative infinity (-inf) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85315033-4954-481e-8c98-d763fd3d5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask   = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d8b5abb-ca52-4413-ba80-13b7d5634ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aee8da-37b4-4130-b760-a62a5c124373",
   "metadata": {},
   "source": [
    "### 3.5.2 masking additional attention weights with dropout\n",
    "- Dropout is a technique where __randomly selected hidden layer units are ignored during training__. This helps prevent overfitting by ensuring that a model does not become overly reliant on a specific set of hidden layer units.\n",
    "- Dropout is only used during training and is disabled afterward.\n",
    "- In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied in two areas:\n",
    "    - After calculating the attention scores\n",
    "    - After applying the attention weights to the value vectors.\n",
    "- Here we will apply the dropout mask after computing the attention weights, as illustrated in figure 3.22, because it’s the more common variant.\n",
    "![fig3-22](px/fig3-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a107dd5-8193-4aeb-afa0-7feea24c807f",
   "metadata": {},
   "source": [
    "- We use a dropout rate of 50% (masking out half of the attention weights) below. (we will use a 0.1-0.2 dropout rate when training GPT.)\n",
    "- We apply PyTorch’s dropout implementation first to a 6 × 6 tensor consisting of ones for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa4701e9-7e57-45e9-885b-243441d63a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e93db-d994-4e69-b20f-ece70813beab",
   "metadata": {},
   "source": [
    "- When applying 50% dropout, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the remaining elements are scaled up by a factor of 1/0.5 = 2.\n",
    "- This maintains the balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc2b224c-9a33-4b3c-a89f-bf572f167fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48828e0b-b84e-4d06-8a5b-21b317b1b6a2",
   "metadata": {},
   "source": [
    "- The results may look different depending on your OS. See the PyTorch issue tracker at [https://github.com/pytorch/pytorch/issues/121595]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef5ed5-e84a-4575-93a2-328e1488ec56",
   "metadata": {},
   "source": [
    "### 3.5.3 - designing a compact causal attention class\n",
    "- let’s ensure the code can handle batches consisting of >1 input so the CausalAttention class supports the batch outputs produced by the data loader we\n",
    "implemented in chapter 2. For simplicity, to simulate such batch inputs, we duplicate the input text example.\n",
    "- It returns a 3D tensor - two input texts with six tokens each, where each token is a three-dimensional embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e5ed8ea-a575-4976-8e11-88d2d758850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103fa49-865b-455d-9f9c-599a1acc04d9",
   "metadata": {},
   "source": [
    "- The __CausalAttention__ class is similar to the __SelfAttention__ class, except that we now added the dropout and causal mask components.\n",
    "\n",
    "- Using PyTorch's __self.register_buffer()__ method is not critical for all use cases but offers several advantages here. For instance, when we use the CausalAttention class in our LLM, __buffers are automatically moved to the appropriate device (CPU or GPU)__ along with our model. This means we don’t need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ae81acb-0bd4-4bbc-9e92-9c79cc4e721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out   = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) #A\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape #C\n",
    "        keys                = self.W_key(x)\n",
    "        queries             = self.W_query(x)\n",
    "        values              = self.W_value(x)\n",
    "        attn_scores         = queries @ keys.transpose(1, 2)\n",
    "        \n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec  = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46940c47-1c24-4ac9-8265-66af6e1e2e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# The resulting context vector is a 3D tensor - each token is represented by a 2D embedding.\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca             = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs   = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439a235-cf5d-474f-ba10-ca123ce2328e",
   "metadata": {},
   "source": [
    "- progress thus far:\n",
    "![fig3-23](px/fig3-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e618f-6ee3-4df5-9e97-bcb2bbfc35b7",
   "metadata": {},
   "source": [
    "## 3.6 extending single-head attention to __multi-head__ attention\n",
    "- “multi-head” refers to dividing the attention mechanism into multiple independent “heads,” each operating independently. A single causal attention module can be considered single-\n",
    "head attention - there is __only one set of attention weights__ processing the input sequentially.\n",
    "- First, we stack multiple CausalAttention modules for illustration purposes. \n",
    "- Second we build a multi-head attention module in a more complicated but more computationally efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3cc2b-5c85-4b32-b032-0cbe7ee7f748",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking multiple single-head attention layers\n",
    "- Using multiple instances of the self-attention mechanism can be computationally intensive, but it’s crucial for the kind of complex pattern recognition that models\n",
    "like transformer-based LLMs are known for.\n",
    "![fig3-24](px/fig3-24.png)\n",
    "- The idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections—the results of multiplying the\n",
    "input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e07f6315-9e5f-441d-abcb-18225c04dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                [CausalAttention(\n",
    "                    d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af02c63-e77e-496b-b4aa-b66e6775fb4c",
   "metadata": {},
   "source": [
    "- If we use this __MultiHeadAttentionWrapper__ class with two attention heads (num_heads=2) and CausalAttention output dimension (d_out=2), this results in 4D\n",
    "context vectors (d_out*num_heads=4).\n",
    "![fig3-25](px/fig3-25.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6e8fcf2-c90e-420e-8db0-70e9a8d910c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out    = 3, 2\n",
    "mha            = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs   = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6281c30-9277-4191-ba86-be6a86049f53",
   "metadata": {},
   "source": [
    "- The 1st dimension of __context_vecs__ is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those).\n",
    "- The 2nd dimension refers to the 6 tokens in each input.\n",
    "- The 3rd dimension refers to the 4D embedding of each token.\n",
    "- In this section, we implemented a __MultiHeadAttentionWrapper__ that combined multiple single-head attention modules. They are processed sequentially via [head(x) for head in\n",
    "self.heads] in the forward method.\n",
    "- We can improve this by processing the heads in parallel, by computing the outputs for all attention heads simultaneously via matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f82e2-e25a-4db1-a574-66f5bc9f4eb3",
   "metadata": {},
   "source": [
    "### 3.6.2 multi-head attention with weight splits\n",
    "- Instead of maintaining two separate classes (MultiHeadAttentionWrapper and CausalAttention), we can combine them into a single __MultiHeadAttention__ class.\n",
    "- We will also make some multi-head attention optimizations.\n",
    "- In __MultiHeadAttentionWrapper__, multiple heads are created by creating a list of __CausalAttention__ objects (self.heads), each representing a separate attention head. The\n",
    "__CausalAttention__ class independently performs the attention mechanism, and the results from each head are concatenated.\n",
    "- __MultiHeadAttention__ will integrate the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77cc9df3-7664-468a-8e1d-4fa3ef82f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out     = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim  = d_out // num_heads # reduces projection dim to match desired output dim\n",
    "        self.W_query   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key     = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj  = nn.Linear(d_out, d_out) # linear layer to combine head outputs\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        #C Tensor shape: (b, num_tokens, d_out)\n",
    "        keys                = self.W_key(x)\n",
    "        queries             = self.W_query(x)\n",
    "        values              = self.W_value(x)\n",
    "\n",
    "        #D implicitly split the matrix by adding a num_heads dimension. \n",
    "        # then unroll the last dim: (b,num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        keys                =    keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values              =  values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries             = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        #E Transpose from (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys    =    keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values  =  values.transpose(1, 2)\n",
    "\n",
    "        #F Computes dot product for each head\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        #G Masks truncated to the number of tokens\n",
    "        mask_bool   = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        #H Uses the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        #I Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "        context_vec  = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        #J Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        #K Adds an optional linear projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d836bdb-fc36-418c-aef8-b9e3886a27a3",
   "metadata": {},
   "source": [
    "![fig3-26](px/fig3-26.png)\n",
    "\n",
    "- The splitting of the query, key, and value tensors is done by tensor reshaping and transposing operations (using PyTorch’s __.view__ and __.transpose__ methods). The\n",
    "input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.\n",
    "- The key operation is to __split the d_out dimension into num_heads and head_dim__ (head_dim = d_out / num_heads).\n",
    "- This splitting is done using __.view__: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens, num_heads, head_dim).\n",
    "- The tensors are then transposed to bring the num_heads dimension before the num_tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim).\n",
    "- This transposition is crucial for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "817ccb3c-9886-4e84-9e35-4192867d21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).\n",
    "a = torch.tensor([[\n",
    "    [\n",
    "        [0.2745, 0.6584, 0.2775, 0.8573],\n",
    "        [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "        [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "    [\n",
    "        [0.0772, 0.3565, 0.1479, 0.5331],\n",
    "        [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "        [0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092be11c-6ea4-48f7-98d8-f033d9f5a1e6",
   "metadata": {},
   "source": [
    "- Perform a batched matrix multiplication between the tensor and a view of the tensor where we transposed the last two dimensions (num_tokens and head_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdb83af9-e7d2-4cb6-a63e-dbded59f6fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccb907-eafa-4470-83ff-f5d75173c9fe",
   "metadata": {},
   "source": [
    "- The matrix multiplication in PyTorch handles the 4D input tensor so that the matrix multiplication is carried out between the two last dimensions\n",
    "(num_tokens, head_dim) and then repeated for the individual heads.\n",
    "- For instance, the preceding becomes a more compact way to compute the matrix multiplication for each head separately.\n",
    "- The results match those using the batched matrix multiplication print(a @ a.transpose(2, 3)) (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9f55bd6-669d-4458-a006-fc6f62eea2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res  = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res  = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db910c25-c45a-4675-bee0-f05a9117ea74",
   "metadata": {},
   "source": [
    "- After computing the attention weights and context vectors, the context vectors from all heads are transposed back to the shape (b, num_tokens, num_heads,\n",
    "head_dim). These vectors are then reshaped (flattened) into the shape (b, num_tokens, d_out), effectively combining the outputs from all heads.\n",
    "- We added a so-called __output projection layer (self.out_proj)__ to MultiHeadAttention after combining the heads, which is not present in the CausalAttention class. This layer is not strictly necessary (see the References section in appendix B for more details), but it is commonly used in many LLM architectures, which is why we added it here for completeness.\n",
    "- Even though MultiHeadAttention looks more complicated than MultiHeadAttentionWrapper, it is more efficient - we only need one matrix multiplication to compute the keys, for\n",
    "instance, keys = self.W_key(x) (the same is true for the queries and values).\n",
    "- MultiHeadAttentionWrapper needs to repeat this matrix multiplication (computationally one of the most expensive steps), for each attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97a04d66-c706-43ec-89b8-47d1f1b65767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out                            = 2\n",
    "mha                              = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b800be-143d-41a2-a287-ac269e7c5ad3",
   "metadata": {},
   "source": [
    "- In this section we built a __MultiHeadAttention__ class. While the code is functional, we used small embedding sizes and numbers of attention heads to keep the outputs\n",
    "readable. The smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1,600.\n",
    "- Note that the embedding sizes of the token inputs and context embeddings are the same in GPT models (d_in = d_out)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260de4e-cfe0-4bd6-aac1-c6e18e90a34f",
   "metadata": {},
   "source": [
    "# 4 Implementing a GPT model from Scratch To Generate Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9d9c5-9b26-4afa-971f-cb0373c958aa",
   "metadata": {},
   "source": [
    "## 4.1 Coding an LLM architecture\n",
    "\n",
    "![fig4-2](px/fig4-2.png)\n",
    "\n",
    "### GPT-2 VS. GPT-3\n",
    "- We are focusing on GPT-2 because OpenAI has made the weights of the pretrained model publicly available - we will load them in chapter 6.\n",
    "- GPT-3 is a similar architecture, scaled from 1.5B params (GPT-2) to 175B params & trained on more data. As of this writing, the weights for GPT-3 are not publicly available.\n",
    "- GPT-2 is also a better choice for learning how to implement LLMs, as it can be run on a single laptop. GPT-3 requires a GPU cluster for training and inference.\n",
    "- According to Lambda Labs, it would take 355 years to train GPT-3 on a single V100 datacenter GPU and 665 years on a consumer RTX 8000 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6b422c0-fe7c-44bc-9014-5aca8e80e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 model configuration (dict)\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd5710-9bfd-4c3b-b8e0-0ee67408015f",
   "metadata": {},
   "source": [
    "- Start by implementing a GPT placeholder architecture (DummyGPTModel)\n",
    "- It defines a simple GPT-like model using PyTorch's __nn.Module__. It consists of token and positional embeddings, dropout, a series of transformer blocks\n",
    "(DummyTransformerBlock), a final layer normalization (DummyLayerNorm), and a linear output layer (out_head).\n",
    "- The configuration is passed in via a Python dictionary.\n",
    "- The forward method describes the data flow through the model: it computes token and positional embeddings for the inputs, applies dropout, processes the data through the\n",
    "transformer blocks, applies normalization, and finally produces logits with the linear output layer.\n",
    "- We are using placeholders (DummyLayerNorm and DummyTransformerBlock) for the transformer block and layer normalization for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17860d1c-45a9-4d00-8aed-937b73a91c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb    = nn.Embedding(cfg[\"vocab_size\"],     cfg[\"emb_dim\"])\n",
    "        self.pos_emb    = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb   = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        #A Uses a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg)\n",
    "            for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        #B Uses a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds          = self.tok_emb(in_idx)\n",
    "        pos_embeds          = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x                   = tok_embeds + pos_embeds\n",
    "        x                   = self.drop_emb(x)\n",
    "        x                   = self.trf_blocks(x)\n",
    "        x                   = self.final_norm(x)\n",
    "        logits              = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "#C A simple placeholder class that will be replaced by a real TransformerBlock later\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    #D does nothing - just returns input.\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "#E placeholder - will be replaced by a real TransformerBlock later\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5): #The parameters here are just to mimic the LayerNorm interface.\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17c928-bddc-4510-86c2-4a5981a432e8",
   "metadata": {},
   "source": [
    "![fig4-4](px/fig4-4.png)\n",
    "\n",
    "- Tokenize a batch of two text inputs for the GPT model using the tiktoken tokenizer introduced in chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91c56050-7520-4d14-823f-e325519d2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0b7fd-e859-4e5f-87d1-9e2aaf1e0029",
   "metadata": {},
   "source": [
    "- Next, initialize a new 124M parameter DummyGPTModel instance and feed it the tokenized batch. The outputs are commonly known as \"logits\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fcee887-81c8-4162-bdf2-03ee4b056187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e08a59-586b-4fa9-b723-590a7e93db77",
   "metadata": {},
   "source": [
    "- The output tensor has two rows corresponding to the two text samples.\n",
    "- Each text sample has four tokens; each token is a 50,257-dimensional vector (matching the size of the tokenizer’s vocabulary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd1d1c-0633-4266-b7eb-bd84aa9da69a",
   "metadata": {},
   "source": [
    "## 4.2 Normalizing activations with layer normalization (LN)\n",
    "- Training is subject to vanishing or exploding gradient problems. They lead to unstable training dynamics and make it difficult for the network to effectively adjust its weights, which means the learning process struggles to find a set of parameters (weights) that minimizes the loss function. \n",
    "- Layer normalization adjusts the activations (outputs) of a layer to have a __mean of 0 and a variance of 1__, (aka unit variance). It speeds up the convergence to effective weights and ensures consistent, reliable training. LN is typically applied before and after the multi-head attention module and before the final output layer.\n",
    "\n",
    "![fig4-5](px/fig4-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "073e64f8-9e0d-4431-8e7e-23fa11a6b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) # create two training examples with five dimensions each\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf2e2f-d18b-4e8b-bdf0-acc6df1e458a",
   "metadata": {},
   "source": [
    "- The consists of a Linear layer followed by a __nonlinear activation function, ReLU__ (short for rectified linear unit), which is a standard activation function.  ReLUs threshold negative inputs to 0, ensuring that a layer outputs only positive values, which explains why the resulting layer output does not contain\n",
    "any negative values. (we will use another, more sophisticated activation function in GPT, which we will introduce in the next section.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e8d8fc3-7292-4432-a956-22eea437d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0d87b-bacc-4425-a8a0-2c40dc8d8023",
   "metadata": {},
   "source": [
    "- __keepdim=True__ in operations like mean or variance calculation ensures the output tensor retains the __same number of dimensions as the input tensor__, even though the operation\n",
    "reduces the tensor along the dimension specified via dim. Without keepdim=True, the returned mean tensor would be a two-dimensional vector [0.1324, 0.2170] instead of a 2 × 1-\n",
    "dimensional matrix [[0.1324], [0.2170]].\n",
    "\n",
    "![fig4-6](px/fig4-6.png)\n",
    "\n",
    "- For a 2D tensor (like a matrix), using dim=-1 for operations such as mean or variance calculation __is the same as using dim=1__. This is because __-1 refers to the tensor’s\n",
    "last dimension__, (columns in a 2D tensor).\n",
    "- Later, when adding layer normalization to the GPT model, which produces 3D tensors with shape [batch_size, num_tokens, embedding_size], we can still use dim=-1 for normalization across the last dimension, avoiding a change from dim=1 to dim=2.\n",
    "- Next: apply LN to the layer outputs we obtained earlier by subtracting the mean and dividing by the square root of the variance (also known as standard deviation). This results in means near zero, and variances of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18bb7862-81ac-4db2-a7f6-40d0c559d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e83dde-b1fa-4ed0-b324-8136f316640d",
   "metadata": {},
   "source": [
    "- (repeating the above with scientific notation disabled):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c6e8afa-d617-4e39-bc1c-5c4992c4892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b5617-18e8-4af2-aabe-2f46691f12c1",
   "metadata": {},
   "source": [
    "- Layer normalization class definition.\n",
    "- This design operates on the last dimension of the input tensor x = embedding dimension (emb_dim).\n",
    "- __eps__ is a small constant (epsilon) added to the variance to prevent division by zero during normalization.\n",
    "- __scale_ and __shift__ are trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model’s performance on its training task. This allows the model to learn appropriate scaling and shifting that best suit the data it is processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "095f24f9-abaa-4e33-9c4d-13d51e51bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps   = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean   = x.mean(dim=-1, keepdim=True)\n",
    "        var    =  x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66938db3-a76f-44c3-ae6b-cdfe08d68624",
   "metadata": {},
   "source": [
    "### BIASED VARIANCE\n",
    "- In our variance calculation method, we set unbiased=False. In the variance calculation we divide by the number of inputs (n). This approach does not apply\n",
    "__Bessel’s correction__, which typically uses n – 1 instead of n in the denominator to adjust for bias in sample variance estimation. This results in a \"biased estimate\" of the variance.\n",
    "- For LLMs, where the embedding dimension n is large, the difference between using n and n – 1 is practically negligible. This approach ensures compatibility with the GPT-2 model’s normalization layers. It also reflects TensorFlow’s default behavior, which was used to implement the original GPT-2 model. Using a similar setting ensures our method is compatible with the pretrained weights we will load in chapter 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1eea1846-7ffe-4f94-b8ab-ef73d32cd8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# apply LN to our batch input\n",
    "ln     = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean   = out_ln.mean(dim=-1, keepdim=True)\n",
    "var    = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7c727-8d2b-4d41-9af9-1631f8a99d05",
   "metadata": {},
   "source": [
    "### Progress thus far:\n",
    "![fig4-7](px/fig4-7.png)\n",
    "\n",
    "### LAYER NORMALIZATION VS. BATCH NORMALIZATION\n",
    "- batch normalization is a common normalization method for neural networks. Unlike batch normalization, which normalizes across the batch dimension, __LN normalizes across the feature dimension__.\n",
    "- LLMs usually need significant computational resources - the hardware or use case can dictate the batch size during training or inference. Since layer normalization normalizes each input independently of the batch size, it offers more flexibility and stability in these scenarios. This is particularly beneficial for distributed training or when deploying models in environments where resources are constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b1476-4b3a-475f-97df-bb9533fbe666",
   "metadata": {},
   "source": [
    "## 4.3 - feed forward networks with GELU activations\n",
    "- In this section, we implement a neural network submodule that is used as part of the transformer block.\n",
    "- Start by designing the GELU activation function. Historically, ReLU has been commonly used due to its simplicity and effectiveness.\n",
    "- Several other activation functions are also often used:\n",
    "    - __GELU__ (Gaussian error linear unit) and __SwiGLU__ (Swish-gated linear unit).\n",
    "- GELU and SwiGLU use Gaussian and sigmoid-gated linear units, respectively. They offer improved performance.\n",
    "- GELU can be implemented in several ways; the exact version is defined as GELU(x) = xΦ(x), where Φ(x) is the cumulative distribution function of the standard Gaussian\n",
    "distribution. In practice, however, it’s common to implement a computationally cheaper approximation (the original GPT-2 model was also trained with this approximation):\n",
    "![GELU](px/GELU.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0686c135-12a1-4a73-ace3-e93c55c134d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0778a30-c544-41a7-8eef-17313b66bc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot ReLU vs GELU\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu     = GELU(), nn.ReLU()\n",
    "x              = torch.linspace(-3, 3, 100) #A\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173ece8-37e2-495c-981c-a3e40d1a8afa",
   "metadata": {},
   "source": [
    "- The smoothness of GELU can lead to better optimization properties during training - it allows for more nuanced adjustments to the model’s parameters. ReLU has a sharp corner at zero, which can sometimes make optimization harder, especially in networks that are very deep or have complex architectures.\n",
    "- GELU returns a small, non-zero output for negative values. This means neurons that receive negative input can still contribute to the learning process, albeit to a lesser extent than positive inputs.\n",
    "- Next: use GELU to build the small neural network module, FeedForward, that we will be using in the LLM’s transformer block later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f5e1523-09a0-4d87-a8a7-75aba6bf6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ccc52-9a93-43db-b552-036f6e8cc775",
   "metadata": {},
   "source": [
    "- __FeedForward__ contains two Linear layers and a GELU activation function. In the 124M parameter GPT model, it receives input batches with tokens that have an embedding size of 768 each via the GPT_CONFIG_124M dictionary where GPT_CONFIG_124M[\"emb_dim\"] = 768.\n",
    "\n",
    "![fig4-9](px/fig4-9.png)\n",
    "\n",
    "- Below: build a FeedForward module with a token embedding size of 768 and feed it a batch input with two samples and three tokens each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2379b345-c1c3-419d-8a55-a6ee9a574c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80669ee-a97c-4f29-b4ed-8203f3625441",
   "metadata": {},
   "source": [
    "- although the input and output dimensions of The FeedForward module are the same, it internally __expands the embedding dimension into a higher-\n",
    "dimensional space through the first linear layer__. This is followed by a nonlinear GELU activation and then a __contraction__ back to the original dimension. This design enables exploration of a richer feature space.\n",
    "\n",
    "![fig4-10](px/fig4-10.png)\n",
    "\n",
    "- The uniformity in input and output dimensions also simplifies the architecture by __enabling the stacking of multiple layers__, as we will do later, without the need to adjust dimensions between them, thus making the model more scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41084fe2-b5fd-47bc-8eb1-6141b2d5f914",
   "metadata": {},
   "source": [
    "## 4.4 Adding shortcut connections\n",
    "- Originally, shortcut connections (aka \"skip\" or \"residual\" connectons) were proposed for computer vision applications to mitigate the challenge of __vanishing gradients__ (the issue where gradients (which guide weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to\n",
    "effectively train earlier layers.\n",
    "\n",
    "![fig4-12](px/fig4-12.png)\n",
    "\n",
    "- Shortcut connections create alternative, shorter paths for the gradient to flow through the network by skipping one or more layers, which is achieved by adding the output of one layer to the output of a later layer. This is why these connections are also known as skip connections. They play a crucial role in preserving the flow of gradients during the backward pass in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d883ef5a-0672-44cc-818d-120431a2154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers       = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x) # get output of current layer\n",
    "            if self.use_shortcut and x.shape == layer_output.shape: # can shortcut be applied?\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b10cbd-48fc-4f3d-abbc-bdd2bc99bcbc",
   "metadata": {},
   "source": [
    "- Build a net __without__ shortcut connections. Each layer is designed to accept an example with three inputs & returns three outputs. The last layer returns a single output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "388c3cdc-3c41-4f95-8b5c-f4703c26fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb23d4-2276-4249-86a0-c75db0c83c7d",
   "metadata": {},
   "source": [
    "- Design a function to find backward pass gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbb4396a-7df5-472f-b220-5d372a15e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    \n",
    "    # Calculate loss based on how close the target and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945b5ac-9a93-4796-bb51-f8391e9d8536",
   "metadata": {},
   "source": [
    "- In the preceding code, we specify a loss function that computes how close the model output and a user-specified target (here, for simplicity, the value 0) are. Then, when calling loss.backward(), PyTorch computes the loss gradient for each layer in the model.\n",
    "- We can iterate through the weight parameters via __model.named_parameters()__.\n",
    "- Suppose we have a 3×3 weight matrix for a given layer. This layer will have 3×3 gradient values, and we print the mean absolute gradient of these 3×3 gradient values to obtain a single gradient value per layer to compare the gradients between layers more easily.\n",
    "- PyTorch's __.backward()__ method computes loss gradients, which are required during model training, without implementing the math for the gradient calculation ourselves.\n",
    "- Below: apply __print_gradients__ to the model without skip connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2a02cf7-4a90-4efe-a239-6ccf9a93cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3007d5-2ec9-4259-b22f-6d9eb1425c35",
   "metadata": {},
   "source": [
    "- apply model __with__ skip connections for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33121462-2033-4d2b-828e-37d38c47f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9a95b-dcd0-4a9c-bf79-0c51c144f48a",
   "metadata": {},
   "source": [
    "- Notice the last layer (layers.4) still has a larger gradient than the other\n",
    "layers. However, the __gradient stabilizes__ as we progress toward the first layer (layers.0) and doesn’t shrink to a vanishingly small value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cddb97-e69e-4418-9de4-267c6f594811",
   "metadata": {},
   "source": [
    "## 4.5 transformer blocks\n",
    "- The transformer block, which is repeated a dozen times in the 124M parameter\n",
    "GPT-2 architecture, combines several concepts: multi-head attention,\n",
    "layer normalization, dropout, feed forward layers, and GELU activations.\n",
    "\n",
    "![fig4-13](px/fig4-13.png)\n",
    "\n",
    "- When a transformer processes an input sequence, each element in the sequence is represented by a fixed-size vector (in this case, 768 dimensions). The operations are designed to transform these vectors in a way that preserves their dimensionality.\n",
    "- The __self-attention__ mechanism in the multi-head attention block __finds relationships between elements__ while the feed forward network modifies the data individually at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "843a980a-2350-468b-bdac-9d884dfff4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=          cfg[\"emb_dim\"],\n",
    "            d_out=         cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=     cfg[\"n_heads\"],\n",
    "            dropout=       cfg[\"drop_rate\"],\n",
    "            qkv_bias=      cfg[\"qkv_bias\"])\n",
    "        \n",
    "        self.ff            = FeedForward(cfg)\n",
    "        self.norm1         = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2         = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x # shortcut for attn block\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "        \n",
    "        shortcut = x # shortcut for feed fwd block\n",
    "        x        = self.norm2(x)\n",
    "        x        = self.ff(x)\n",
    "        x        = self.drop_shortcut(x)\n",
    "        x        = x + shortcut # add back original input\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff056fff-a4d7-4c6a-8c5e-1317dead304a",
   "metadata": {},
   "source": [
    "- This defines a TransformerBlock class that includes a multi-head attention\n",
    "mechanism (__MultiHeadAttention__) and a feed forward network (__FeedForward__).\n",
    "- Layer normalization (__LayerNorm__) is applied before each of these two components, and dropout is applied afterwards to regularize the model and prevent overfitting. This is also known as Pre-LayerNorm.\n",
    "- Older architectures, such as the original transformer model, applied layer normalization after the self-attention and feed forward networks instead, known as Post-LayerNorm, which often leads to worse training dynamics.\n",
    "- The class also implements the __forward pass__, where each component is followed by a shortcut connection that adds the input of the block to its output.\n",
    "- Below: instantiate a transformer block and feed it some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7d2fc74-a631-461e-a1ba-de0cfd4c14c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x      = torch.rand(2, 4, 768)\n",
    "block  = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a19218-9947-4032-8120-6d5910c8a2ab",
   "metadata": {},
   "source": [
    "- The preservation of shape throughout the transformer block is crucial. It enables uses across a wide range of sequence-to-sequence tasks, where each output vector directly corresponds to an input vector.\n",
    "- The output is a context vector that __holds information from the entire input sequence__. This means that while the length and feature size remain unchanged, the content of each output vector is re-encoded to integrate\n",
    "contextual information from across the entire input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eba29a-d9f1-4fbc-b5e4-c562e028905b",
   "metadata": {},
   "source": [
    "## 4.6 Coding the GPT model\n",
    "- We started with aN overview of a GPT architecture (\"DummyGPTModel\"). We showed the inputs and outputs, but its building blocks remained a black box using placeholder __DummyTransformerBlock__ and __DummyLayerNorm__ classes.\n",
    "\n",
    "![fig4-15](px/fig4-15.png)\n",
    "\n",
    "- Note how the transformer block is repeated many times throughout the model. - It’s repeated 12 times (specified by __n_layers__ in the config) in GPT-2 (124M params), and 36 times in GPT-2 with 1542M params.\n",
    "- The output from the final transformer block is fed to a final layer normalization step before reaching the linear output layer. This layer maps the transformer’s output to a high-dimensional space (in this case, 50,257 dimensions, corresponding to the model’s vocabulary size) to predict the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99015e18-0533-4f8a-a84a-e9201e957acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb    = nn.Embedding(cfg[\"vocab_size\"],     cfg[\"emb_dim\"])\n",
    "        self.pos_emb    = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb   = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds          = self.tok_emb(in_idx)\n",
    "        # device setting allows us to choose CPU or GPU\n",
    "        pos_embeds          = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0596238-1bce-4670-9654-80ee704e5b73",
   "metadata": {},
   "source": [
    "- Thanks to the TransformerBlock class, the GPTModel class is compact.\n",
    "- __init__ initializes the token and positional embedding layers using the config dict. The embedding layers convert input token indices into dense vectors and adding positional information (see chapter 2).\n",
    "- Next, __init__ creates a sequential stack of TransformerBlock modules equal to the number of layers specified in cfg.\n",
    "- Then a __LayerNorm__ layer is built. It standardizes the outputs from the transformer blocks to stabilize learning.\n",
    "- Finally, a __linear output head__ without bias is defined. It projects the transformer’s output into the vocabulary space of the tokenizer to generate logits for each token in the vocabulary.\n",
    "- __forward__ does the following:\n",
    "    1. computes their embeddings of a batch of input token indices\n",
    "    2. applies the positional embeddings\n",
    "    3. passes the sequence through the transformer blocks\n",
    "    4. normalizes the final output\n",
    "    5. computes the logits (the next token’s unnormalized probabilities)\n",
    "- We will convert these logits into tokens and text outputs in the next section.\n",
    "- Below: initialize the 124M param GPT model using the config dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de322d71-969e-48cb-91a6-e36ee2592e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3c55e-2446-4264-a173-055ca11f7cc6",
   "metadata": {},
   "source": [
    "- As we can see, the output tensor has the shape [2, 4, 50257]. (we passed two input texts with four tokens each.) The last dimension, 50,257, corresponds to the vocabulary size of the tokenizer.\n",
    "- let’s analyze the size of the model architecture.\n",
    "- __numel()__ (“number of elements”) will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4aa71517-419e-46a2-bb6b-e255b70de96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840aac8-d45c-4338-811d-7b6a2ea5e0e8",
   "metadata": {},
   "source": [
    "- Why is the actual number of parameters 163M, not 124M?\n",
    "- The reason is a concept called __weight tying__ used in the original GPT-2 architecture. It means the architecture is __reusing the weights__ from the token embedding layer in its output layer.\n",
    "- To understand what this means, let’s take a look at the shapes of the token\n",
    "embedding layer and linear output layer that we initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1131159-8e4d-4cc9-9a22-4f5ea6977c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50de82-3671-436f-9818-1ec62cf1dc36",
   "metadata": {},
   "source": [
    "- The token embedding and output layers are very large due to the number of rows (50,257) in\n",
    "the tokenizer’s vocabulary. Let’s remove the output layer parameter count from the total GPT-2 model count according to the weight tying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "16116cd0-8c84-4f7a-995b-39db5838f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters()))\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb574ff3-2db4-4759-bfb6-2ac4f6eb90ef",
   "metadata": {},
   "source": [
    "- Now the model has 124M parameters - matching the original size.\n",
    "- __Weight tying reduces memory footprint and computational complexity__.\n",
    "- However, in my experience, using separate token embedding and output layers results in better training and model performance; hence, we use separate layers in our GPTModel implementation.\n",
    "- The same is true for modern LLMs. However, we will revisit and implement the weight tying concept later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e676d59-9a18-4700-abb4-c97bcbe41df0",
   "metadata": {},
   "source": [
    "- Find the memory requirements of the 163M parameters in our GPTModel\n",
    "object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "946b75d1-3983-41ca-8bc7-8d50df759fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# assumes using float32 = 4 bytes per parameter\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18da72-9024-49d0-a36d-95f809d26367",
   "metadata": {},
   "source": [
    "## 4.7 Generating text\n",
    "\n",
    "![fig4-16](px/fig4-16.png)\n",
    "\n",
    "- With each iteration, the input context grows & allows the model to generate appropriate text. By the sixth iteration, the model has constructed a complete sentence: “Hello, I am a model ready to help.”\n",
    "- In the previous section, we saw that our current GPTModel implementation outputs tensors with shape [batch_size, num_token, vocab_size]. So how does a GPT model go from these output tensors to the generated text?\n",
    "- In each step, __the model outputs a matrix with vectors representing potential next tokens__. The vector corresponding to the next token is extracted and converted into a probability distribution via softmax function.\n",
    "- Within the vector containing the resulting probability scores, __the index of the highest value is located, which translates to the token ID__. This token ID is then decoded back into text, producing the next token in the sequence.\n",
    "- Finally, this token is __appended to the previous inputs__, forming a new sequence for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b297c98-ebbb-497d-a9c4-59f5d7f94b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(\n",
    "    model, idx, # array of (batch, n_tokens) indices in current context\n",
    "    max_new_tokens, context_size):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # crop current context if larger than supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # focus on last time step, so (batch, n_token, vocab_size)\n",
    "        # becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # probas has shape (batch, vocab_size)\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # idx_next has shape (batch,1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        # appends sampled index to running sequence.\n",
    "        # idx has shape (batch, n_tokens+1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae185ea-30e7-41d1-9141-e7e72777b50a",
   "metadata": {},
   "source": [
    "- This function iterates for a specified number of new tokens to be generated,\n",
    "crops the current context to fit the model’s maximum context size, computes predictions, and then selects the next token based on the highest probability prediction.\n",
    "- We use softmax to convert logits into a probability distribution from which we identify the position with the highest value via __torch.argmax__. softmax is monotonic (it preserves the order of its inputs when transformed into outputs). So, in practice, the softmax step is redundant since the position with the highest score in the softmax output tensor is the same position in the logit tensor. In other words, __we could apply torch.argmax to the logits tensor directly and get identical results.\n",
    "- However, this illustrates the full process of transforming logits to probabilities, which can add additional intuition so that the model generates the most likely next\n",
    "token, which is known as __greedy decoding__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f01511c-24f6-4aa4-9a79-f858f2da956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) # adds batch dimension\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f2b4c-56a9-4864-8cf6-64903481abe9",
   "metadata": {},
   "source": [
    "- Use __.eval() mode__ which disables random components like dropout, which are only used during training, and use the __generate_text_simple__ function on the encoded input\n",
    "tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6063c04-b116-4ea0-a41f-47f8a6d8cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657]])\n",
      "Output length: 14\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disables dropout, since we aren't training\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f79ec2-81e2-45b7-9840-a0e94c9cb130",
   "metadata": {},
   "source": [
    "- Convert the IDs back into text using __.decode__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "154ab0d9-91cb-412f-b30c-e814c6fea471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d05ddd-383e-4911-aa4a-40de555a28f9",
   "metadata": {},
   "source": [
    "- The model generated gibberish. What happened? The reason is that we haven’t trained it yet. So far, we have designed a GPT architecture and initialized a GPT model instance with __initial random weights__. We will tackle model training in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b4fbb-2322-42b0-89f2-6a7694a736bf",
   "metadata": {},
   "source": [
    "# 5 Pretraining on Unlabeled Data\n",
    "### WEIGHT PARAMETERS\n",
    "- Weights refer to trainable (learnable) parameters. In frameworks like PyTorch, weights are stored in __linear layers__; we used these to build a __multi-head attention__ module and\n",
    "the __GPTModel__ in chapter 4.\n",
    "- __torch.nn.Linear(...))__ let us access its weights via __new_layer.weight__.\n",
    "- PyTorch allows direct access to all a model’s trainable parameters, including\n",
    "weights and biases, via __model.parameters()__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceecd9d-3ef2-4d6e-b2a9-436d85d50082",
   "metadata": {},
   "source": [
    "## 5.1 Evaluating generative text models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611647df-fec7-46fd-8551-1afb36475619",
   "metadata": {},
   "source": [
    "### 5.1.1 Using GPT to generate text\n",
    "- Begin by initializing a GPT model using __GPTModel__ and __GPT_CONFIG_124M__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee3b4c89-ba05-4289-bda1-d7e4bff15602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import torch\n",
    "#from chapter04 import GPTModel\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, # shortened from 1024\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1, # common setting: 0\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8cd1d-5441-48e6-b365-4ecf62abbbbc",
   "metadata": {},
   "source": [
    "- The only adjustment we made compared to the previous chapter is reducing the context length (context_length) to 256 tokens. This reduces the computational demands of training the model, making it possible to carry out the training on a standard laptop.\n",
    "- Originally, GPT-2 with 124M params was configured to handle up to 1,024 tokens. After training we will update the context size & load pretrained weights to work with a model with a 1,024-token context length.\n",
    "- Using the __GPTmodel__ instance, we adopt __generate_text_simple__ and introduce __text_to_token_ids__ and __token_ids_to_text__.\n",
    "\n",
    "![fig5-3](px/fig5-3.png)\n",
    "\n",
    "- This illustrates a three-step text generation process using a GPT model.\n",
    "1) tokenizer converts input text into a series of token IDs (chap 2).\n",
    "2) model gets token IDs and generates corresponding logits (vectors of probability\n",
    "distribution for each token in the vocabulary (chap 4).\n",
    "3) ogits are converted back to token IDs - the tokenizer decodes them into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "17debd1a-c85f-4a93-b267-af9a4bf7bbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "#from chapter04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    # .unsqueeze(0) adds the batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer     = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids     = generate_text_simple(\n",
    "    model=model, \n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f536541-5048-449b-9857-4d701e2fd46f",
   "metadata": {},
   "source": [
    "- It’s clear the model isn’t yet producing coherent text. To define what makes text “coherent” or “high quality,” we have to design a numerical method to evaluate the generated content. It's called a __loss_metric__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c2cc9-16e1-4b0a-924e-c64c71f4d12b",
   "metadata": {},
   "source": [
    "### 5.1.2 Calculating the text generation loss\n",
    "- Consider the two input examples, which have already been mapped to token IDs and matching targets (what we want the model to produce.)\n",
    "- Note: targets = inputs, shifted one position forward. Crucial for prediction training.\n",
    "- Feed the inputs into the model to find logits vectors. (2 x 3 tokens each.)\n",
    "- Apply softmax on logits - returns prob scores (probas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b465ae0e-4d92-4a67-a9ab-edc2f11f7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "                        [40, 1107, 588]]) # \"I really like\"]\n",
    "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
    "                        [588, 428, 11311]]) # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c035ca01-3623-4778-8bea-ecaf8e169763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # disable gradient tracking (not training yet)\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1) #prob of each token in vocabulary\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c12f5-48bf-4c58-a74d-1f5af98ffc66",
   "metadata": {},
   "source": [
    "- 2 = two examples in the inputs = batch size.\n",
    "- 3 = number of tokens per input.\n",
    "- 50257 = embedding dimension = vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bfc6eb2c-3e72-4351-a449-2fe04e53448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bd027730-80a1-473a-9649-2e67415add3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# convert token IDs back to text - not yet trained, so we get random text back.\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "    f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e7481-1a91-4045-8245-7a51428b6f15",
   "metadata": {},
   "source": [
    "![fig5-6](px/fig5-6.png)\n",
    "- Text evaluation measures “how far” the tokens are from the correct predictions (targets).\n",
    "Training functions use the distance to adjust model weights by increasing the softmax probability in the index positions corresponding to the correct target tokens. Softmax is also used to assess\n",
    "the model’s generated outputs: the higher the probability in the correct positions, the better.\n",
    "- The above figure displays __softmax values for a simple seven-token vocabulary__. This implies the starting random values will hover around 1/7 (~0.14). \n",
    "- Our GPT-2 model vocabulary has 50,257 tokens, so the initial probabilities will start at ~0.00002.\n",
    "- For each of the two input texts, print the initial softmax probability scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f1b5dc59-b7fd-4650-80aa-a8ee6b7cea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf3845-4bed-42db-90ff-af776fe67758",
   "metadata": {},
   "source": [
    "### BACKPROPAGATION\n",
    "- How do we maximize the softmax values corresponding to target tokens? The weight update is done via\n",
    "__backpropagation__). It requires a loss (distance) function and the desired output.\n",
    "- Let's calculate the loss for the probability scores of _target_probas_1_ and _target_probas_2_.\n",
    "\n",
    "![fig5-7](px/fig5-7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e0607d42-a3e2-484d-86a0-f57053d1fce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -10.1307, -10.9951, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a29b9-a59f-441b-bf2e-4e252638ea80",
   "metadata": {},
   "source": [
    "- Working with logarithms is more manageable in optimization than handling the scores directly. Combine log probabilities into a single average score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fa617763-5a41-485c-8400-f96126058ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7722)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb073892-feed-4176-8833-94f38fe560c1",
   "metadata": {},
   "source": [
    "- The goal is to get the average log probability as close to 0 as possible. The common practice isn’t to push the average log probability up to 0 but to __bring the negative average log probability down to 0__. The negative average log probability is simply the average log probability multiplied by –1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7bb540a5-76ea-4e03-b3c8-6b91aff5e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54dc94-813d-4972-9b19-b09260da069a",
   "metadata": {},
   "source": [
    "- The term for this negative value, –10.7722, turning into 10.7722, is known as __cross entropy loss__. PyTorch already has a built-in cross_entropy function that takes care of all these six steps in figure 5.7 for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5f4ec-652d-4a52-a6cc-1b0e2966e652",
   "metadata": {},
   "source": [
    "### CROSS ENTROPY LOSS\n",
    "- Cross entropy loss __measures the difference between two probability distributions__ —typically, the\n",
    "__true distribution__ of labels (tokens) and the __predicted distribution__ from a model.\n",
    "- PyTorch's cross_entropy function computes this measure for __discrete outcomes__, which is similar to the __negative average log probability__ of the target tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c023fd40-1b08-47b6-bf95-79e07ec2b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# review the shapes of the logits and target tensors\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87c9af-c352-4781-8707-3cd839cfcf85",
   "metadata": {},
   "source": [
    "- For PyTorch cross_entropy loss function: flatten tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8b51c79-b47a-4cd1-8ebc-50c430537aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat  = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533d037-ec53-4644-97c4-3bd9918659b0",
   "metadata": {},
   "source": [
    "- Remember: the targets are the token IDs we want the LLM to generate. The logits contain\n",
    "the __unscaled model outputs__ before they enter the softmax function to obtain probability scores.\n",
    "- Previously, we applied softmax, selected the probability scores corresponding to the target IDs, and computed the negative average log probabilities. PyTorch’s cross_entropy function will take care of all these steps for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c3f0fa18-9ed2-43a5-a8e5-dfb3cb913aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808bb06-75cb-4236-af66-ecdc8eac5188",
   "metadata": {},
   "source": [
    "### PERPLEXITY\n",
    "- Perplexity __measures how well the probability distribution predicted by the model matches\n",
    "the actual distribution of the words in the dataset__. Lower perplexity indicates that the model predictions are closer to the actual distribution.\n",
    "- Perplexity can be calculated as perplexity = torch.exp(loss), which returns tensor(47678.8633)\n",
    "when applied to the previously calculated loss.\n",
    "- Perplexity is more interpretable than the raw loss value because __it signifies the effective vocabulary size__ about which the model is uncertain at each step. In the given example, this would translate to the model being unsure about which among 47,678 words or tokens in the vocabulary to generate as the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013a254-0fb1-4458-9741-cc638899bd92",
   "metadata": {},
   "source": [
    "### 5.1.3 Calculating the training and validation set losses\n",
    "\n",
    "- To compute loss on training and validation datasets, we use again use “The Verdict” short story by Edith Wharton. Using a small dataset enables execution of code examples on a laptop computer in a matter of minutes, even without a high-end GPU.\n",
    "\n",
    "### THE COST OF PRETRAINING LLMS\n",
    "\n",
    "- Consider the training of the 7B parameter Llama 2 model, a relatively popular openly available LLM. It needed 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion tokens. At the time of writing, running an 8 × A100 cloud server on AWS costs around $30 per hour. A rough estimate puts the total training cost of such an LLM at around $690,000 (calculated as 184,320 hours divided by 8, then multiplied by $30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d989af57-460f-4801-8a4b-e27a315bb6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20480\n",
      "Tokens: 5146\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032b75e-fe43-40a9-92b9-ce0a34e42d90",
   "metadata": {},
   "source": [
    "- Next, divide the dataset into a training and a validation set and use a data loader to prepare the batches for LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "92e5b53e-c8dd-4a93-b715-ce3c2cffbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx  = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data   = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1494c675-3ecd-4b77-a1e9-f349e789e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc5567e-b6a6-4eb4-9ce7-f8546bf02c88",
   "metadata": {},
   "source": [
    "- We __used a small batch size__ to reduce the computations because we were working with a small dataset. In practice, training LLMs with batch sizes of 1,024 or larger is common. As an optional check, we can iterate through the data loaders to ensure that they were created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e5a6c1a-52a5-4522-a3c4-dd3029aec0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c729222-2e19-4cfb-ac56-e02075ab0032",
   "metadata": {},
   "source": [
    "- We have __nine training set batches__ with two samples and 256 tokens each.\n",
    "- Since we allocated 10% of the data for validation, there is only one validation batch consisting of two input examples.\n",
    "- As expected, the input data (x) and target data (y) have the same shape (the batch size times the number of tokens in each batch) since the targets are the inputs shifted by one position, as discussed in chapter 2.\n",
    "- Next: design a utility function to find the cross entropy loss of a given batch returned via the training and validation loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "abebec84-4de6-4352-9b26-4575e638d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch  = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits       = model(input_batch)\n",
    "    loss         = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), \n",
    "        target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fd942-7659-42bb-8a8e-f985e5e263f7",
   "metadata": {},
   "source": [
    "- We can now use __calc_loss_batch__ to compute the loss for a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "418f96cd-ba18-4b3a-a47a-11d096f86416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "\n",
    "        # iterates over all batches, if num_batches isn't specified.\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # reduces #batches to match #batches in the loader, if #batches > #batches in the loader.\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches # average loss across all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c36810b3-b5ec-4779-b96c-4b56a32500b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.983160018920898\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad(): # disable gradient tracking (not training yet)\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss   = calc_loss_loader(  val_loader, model, device)\n",
    "    \n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b2d74-e9bb-4565-8565-be13022ce2a2",
   "metadata": {},
   "source": [
    "- The losses are high because the model has not been trained. For comparison, the\n",
    "loss approaches 0 if the model learns to generate the next tokens as they appear in the training and\n",
    "validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066716f-b483-4a92-9a2a-2461c095b32c",
   "metadata": {},
   "source": [
    "## 5.2 Training an LLM\n",
    "![fig5-11](px/fig5-11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e53270af-8d07-46d3-8f86-1be92833f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step                    = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()               # resets loss gradients from previous epoch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()                     # find loss gradient\n",
    "            optimizer.step()                    # update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:    # optional evaluation step\n",
    "\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss   {train_loss:.3f}, \"\n",
    "                    f\"Val loss     {val_loss:.3f}\"\n",
    "                )\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a8051-5d24-4eea-bcaa-8f3942cf4f70",
   "metadata": {},
   "source": [
    "- __evaluate_model__ corresponds to step 7 above. It prints the training & validation set losses after each model update so we can evaluate whether the training improves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c39f59c5-3b57-4eda-8764-142b1fdd39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # dropout disabled during evaluation for stable results.\n",
    "    with torch.no_grad(): # gradient tracking disabled (not required during evaluation).\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss   = calc_loss_loader(  val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295b2a4-7e27-4301-acf3-66993f73c455",
   "metadata": {},
   "source": [
    "- __generate_and_print_sample__ tracks whether the model improves during training. It converts a text snippet (start_context) into token IDs & feeds it to the LLM to generate a text sample using __generate_text_simple__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "75348f0b-a9bb-4837-8e49-f2119ed77c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded      = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, \n",
    "            idx=encoded,\n",
    "            max_new_tokens=50, \n",
    "            context_size=context_size)\n",
    "        \n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb970e0-dc83-49d3-835d-3c3b58240230",
   "metadata": {},
   "source": [
    "### ADAMW\n",
    "- Adam optimizers are a popular choice for training deep neural networks. __AdamW__ is a variant of Adam that __improves the weight decay approach__ to minimize model complexity and prevent overfitting by penalizing larger weights. This allows AdamW to achieve more effective regularization and better generalization; thus, AdamW is frequently used in the training of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9cb5fa60-0afa-4eb0-9ad5-91eda7006db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss   9.830, Val loss     9.927\n",
      "Ep 1 (Step 000005): Train loss   8.133, Val loss     8.335\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss   6.770, Val loss     7.048\n",
      "Ep 2 (Step 000015): Train loss   6.497, Val loss     6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss   5.579, Val loss     6.492\n",
      "Ep 3 (Step 000025): Train loss   4.732, Val loss     6.387\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step 000030): Train loss   5.284, Val loss     6.361\n",
      "Ep 4 (Step 000035): Train loss   3.855, Val loss     6.258\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss   3.667, Val loss     6.197\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step 000045): Train loss   3.600, Val loss     6.140\n",
      "Ep 6 (Step 000050): Train loss   2.383, Val loss     6.111\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
      "Ep 7 (Step 000055): Train loss   2.336, Val loss     6.137\n",
      "Ep 7 (Step 000060): Train loss   2.696, Val loss     6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      "Ep 8 (Step 000065): Train loss   1.596, Val loss     6.176\n",
      "Ep 8 (Step 000070): Train loss   1.346, Val loss     6.178\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
      "Ep 9 (Step 000075): Train loss   1.100, Val loss     6.278\n",
      "Ep 9 (Step 000080): Train loss   0.672, Val loss     6.283\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss   0.531, Val loss     6.326\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# takes ~5 mins to finish to typical MacBook\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, \n",
    "    weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, \n",
    "    eval_freq=5, \n",
    "    eval_iter=1,\n",
    "    start_context=\"Every effort moves you\", \n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb211e-96e1-4c27-989f-a4a59fef2f4d",
   "metadata": {},
   "source": [
    "- The training loss improves drastically. The language skills of the model also improved. In the beginning, the model is only able to append commas to the start context (Every effort moves you,,,,,,,,,,,,) or repeat the word and. By the end it can generate grammatically correct text.\n",
    "- Plot the training & validation losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1d651808-3e85-4d35-8a75-5832bbe8cd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses,               label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax2 = ax1.twiny()                            # creates second x-axis; shares same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b897b-f4ee-4d8f-9300-2009f876263a",
   "metadata": {},
   "source": [
    "- The training and validation losses improve during the first epoch, but diverge past the second epoch. This __divergence and the fact that the validation loss is much larger than the training loss indicate that the model is overfitting__ to the training data.\n",
    "- We can confirm that the model is memorizing the training data by searching for the generated text snippets. This is expected - we are using a very small training dataset and training the model for multiple epochs. __Usually, it’s common to train a model on a much larger dataset for only one epoch.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52f407-9100-4123-8bd3-9370456eec66",
   "metadata": {},
   "source": [
    "## 5.3 Decoding strategies to control randomness\n",
    "- Begin by transferring the model back from the GPU to the CPU since inference with a\n",
    "relatively small model does not require a GPU. Also, after training, we put the model into evaluation\n",
    "model to turn off random components such as dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "669ad465-5a92-47eb-b52a-87426f99ea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5919c06-26bc-4ff2-a4bb-2e8e085fdc17",
   "metadata": {},
   "source": [
    "- Plug the GPT model instance into __generate_text_simple__, which uses the LLM to generate one token at a time.\n",
    "\n",
    "- The generated token is selected at each generation step based on the largest probability score among all tokens in the vocabulary. This means the LLM will generate the same outputs even if we run  __generate_text_simple__ multiple times on the same start context (Every effort moves you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f5d321f-c14b-4d74-95fd-33524f498fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model         = model,\n",
    "    idx           = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens= 25,\n",
    "    context_size  = GPT_CONFIG_124M[\"context_length\"])\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2247ff-340e-4db7-9131-4281831bde22",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling (TS)\n",
    "- TS adds a probabilistic selection process to the next-token generation task.\n",
    "- Previously, __generate_text_simple__ always sampled the token with the highest probability as the next token using __torch.argmax__ (aka \"__greedy decoding__\").\n",
    "- We can replace argmax with a sampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f39a7372-2f40-44ab-ba35-f94a7ac7dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "\"closer\": 0,\n",
    "\"every\": 1,\n",
    "\"effort\": 2,\n",
    "\"forward\": 3,\n",
    "\"inches\": 4,\n",
    "\"moves\": 5,\n",
    "\"pizza\": 6,\n",
    "\"toward\": 7,\n",
    "\"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c7264-e6c3-4054-b95e-3ab98213ef4e",
   "metadata": {},
   "source": [
    "- Assume the LLM is given \"every effort moves you\" and geneates the following next-token logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0ecfc7d0-e1a4-48d6-9926-f6c80bad5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39aa144-5b56-48aa-b04c-45f8d81337af",
   "metadata": {},
   "source": [
    "- __generate_text_simple__ converts logits into probabilities via softmax function and gets the token ID corresponding to the generated token via __argmax__, which we can map back into text via the inverse vocabulary.\n",
    "- Since the largest logit value, and correspondingly the largest softmax score, is in the fourth position (index position 3 since Python uses 0 indexing), the generated word is 'forward'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa2ac94f-7f3d-4b66-a72d-b0aa003a4883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas        = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc550ab-29dd-4e1d-a2b5-8631127fe45a",
   "metadata": {},
   "source": [
    "- Now replace argmax with PyTorch's __multinomial function__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "65f456f5-19a0-4698-ae05-ff6094d03b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43452228-fc05-4fc4-ac82-bf5ee347a650",
   "metadata": {},
   "source": [
    "- __multinomial__ samples the next token proportional to its probability score. In other words, \"forward\" is still the most likely token and will be selected by multinomial most of the time but not all the time. To illustrate this, let’s implement a function that repeats this sampling 1,000 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bf667105-a9f5-4a78-b52c-56b478a33bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "        for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcacf7-6a92-4584-8c99-82e1cb3ad73d",
   "metadata": {},
   "source": [
    "- We can control the distribution and selection process via temperature scaling. This is just a fancy description for dividing the logits by a number greater than 0.\n",
    "- Temperatures __greater than 1 return more uniformly distributed token probabilities__.\n",
    "- Temperatures __smaller than 1 return more confident (sharper or more peaky) distributions__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8cf55390-4b50-45d9-9e6b-2c8eb2473021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ffb7b37-7743-4264-a734-bbcf33f5da83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures  = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "    for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "    bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49825895-1d0d-4d31-adfe-47ade3f71193",
   "metadata": {},
   "source": [
    "- A temperature of 1 divides the logits by 1 before passing them to the softmax function to compute\n",
    "the probability scores. (In other words, temperature of 1 is the same as not using any\n",
    "temperature scaling.)\n",
    "- In this case, the tokens are selected with a probability equal to the original softmax probability scores via PyTorch's multinomial sampling function.\n",
    "- For a temperature setting of 1, the token corresponding to “forward” would be selected about 60% of the time.\n",
    "- Applying small temperatures such as 0.1 returns \"sharper\" distributions - the multinomial function selects \"forward\") almost 100% of the time - approaching the behavior of the argmax function.\n",
    "- A temperature of 5 results in a __more uniform distribution__. This can add variety to the generated texts but can return nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57446c60-f050-4383-9355-24234cec2871",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k sampling\n",
    "- Temperature scaling can return nonsensical outputs. Top-k sampling, when combined with\n",
    "probabilistic sampling and temperature scaling, can improve  text generation results.\n",
    "- Top-k sampling __restricts the sampled tokens to the top-k most likely tokens__ & excludes all others.\n",
    "\n",
    "![fig5-15](px/fig5-15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb5d18a8-f525-4417-9d70-2694f89d1c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d3589683-833a-4a14-a24c-98ea34238803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# get next-token logits in this nine-token vocabulary\n",
    "new_logits = torch.where(\n",
    "    condition = next_token_logits < top_logits[-1],\n",
    "    input     = torch.tensor(float('-inf')),\n",
    "    other     = next_token_logits)\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "090056be-9b64-43b4-9675-aa8250846735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# apply softmax to get next-token probabilities\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58970e6-2b96-4712-b921-219a0c663396",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function\n",
    "- The previous two sections introduced temperature sampling and top-k sampling. Now modify the __generate_simple__ into a new __generate__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83cb8cff-79fb-4fec-afce-ac3450eed4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, \n",
    "             context_size, \n",
    "             temperature=0.0, \n",
    "             top_k=None,\n",
    "             eos_id=None): # eos_id missing from book v7 code listing, but in github.\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:              # filter logits with top_k sampling\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val       = top_logits[:, -1]\n",
    "            logits        = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits   = logits / temperature\n",
    "            probs    = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "47578cee-5c7f-4583-a6d2-5adca2185e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to my surprise, a little it was the\n",
      "\"Ah enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model          = model,\n",
    "    idx            = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens = 15,\n",
    "    context_size   = GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k          = 25,\n",
    "    temperature    = 1.4)\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b35ddf-e686-4c49-af50-4c55a779c952",
   "metadata": {},
   "source": [
    "### 5.4 Loading and saving model weights in PyTorch\n",
    "- Pretraining even small LLMs is expensive. Thus, we want to save the LLM so that we don’t have to rerun the training every time we want to use it in a new session.\n",
    "- Saving a PyTorch model is straightforward. The recommended way is to save a model’s __state_dict__, a dictionary mapping each layer to its parameters, using __torch.save__.\n",
    "- After saving the model weights we can load them into a new GPTModel model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "137281eb-816e-4c84-8a3d-dd4a402b54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "de38b4c4-8836-4569-b821-a3a07e0d0129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"my_model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61877241-4e8d-4c45-869b-a167d5b1b689",
   "metadata": {},
   "source": [
    "- Recall how dropout helps prevent overfitting by randomly “dropping out” of a layer’s neurons during training. We don’t want to lose any learned information during inference. __model.eval()__ switches the model to evaluation mode for inference, disabling the dropout layers of the model.\n",
    "- __If we plan to continue pretraining a model later__ — for example, using  __train_model_simple__\n",
    "— we should also save the optimizer state.\n",
    "- Adaptive optimizers such as __AdamW__ store additional parameters for each model weight. AdamW\n",
    "uses historical data to adjust learning rates for each model parameter dynamically. Without it, the\n",
    "optimizer resets, and the model may learn suboptimally or even fail to converge properly, which\n",
    "means it will lose the ability to generate coherent text. Using __torch.save__, we can save both the\n",
    "model and optimizer state_dict contents as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6ee69fee-de49-4bee-983e-f6ddccba272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d2567-3ff3-4c24-8d17-f39f4df767dc",
   "metadata": {},
   "source": [
    "- Then we can restore the model and optimizer states by loading the saved data via\n",
    "__torch.load__, then using __load_state_dict__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "149231e1-bdf5-4834-909a-df71ab578949",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model      = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86539a-5dab-4c4e-afb9-a93b44adab48",
   "metadata": {},
   "source": [
    "## 5.5 Loading pretrained weights from OpenAI\n",
    "- We trained a small GPT-2 model using a limited dataset. This allowed us to focus on the fundamentals without the need for extensive time and computational resources.\n",
    "- OpenAI shared the weights of their GPT-2 models, thus eliminating the need for retraining.\n",
    "- OpenAI originally saved the GPT-2 weights via TensorFlow, which we have to install to\n",
    "load the weights in Python. The following code will use a __progress bar tool called tqdm__ to\n",
    "track the download process, which we also have to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cc1cdb24-8661-4438-9e8c-048438d600f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f66fa-fd92-4c15-bdd1-513f1e7d782d",
   "metadata": {},
   "source": [
    "- The download code is relatively long, mostly boilerplate, and not very interesting. Download the gpt_download.py Python module directly from this chapter’s online repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c42c4c3f-aa2d-4bb8-b0ec-d90fe79fa3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7c0e93b67d40>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch05/\"\n",
    "       \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8c656-6bcb-4c61-8726-68df13cc26e3",
   "metadata": {},
   "source": [
    "- Inspect the contents of this file to ensure that it was saved correctly and contains valid Python code.\n",
    "- Import the __download_and_load_gpt2__ function from the gpt_download.py file as follows, which will load the GPT-2 architecture settings (settings) and weight parameters (params) into our Python session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3e454690-7381-4f71-9791-0926a5ab3a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 09:13:15.529254: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-02 09:13:15.745602: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 09:13:15.826039: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 09:13:15.826652: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 09:13:15.963209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 09:13:16.703789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", \n",
    "    models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557fda1-f193-4300-b532-3267285c0a56",
   "metadata": {},
   "source": [
    "- inspect the contents of settings and params. Both are Python dictionaries.\n",
    "- __settings__ stores the LLM architecture settings similarly to our manually defined GPT_CONFIG_124M settings. __params__ contains the actual weight tensors.\n",
    "- We only printed the dictionary keys because printing the weight contents would take up too much screen space. We can inspect these weight tensors by printing the whole dictionary via __print(params)__ or by selecting individual tensors via the respective dictionary keys, for example, the embedding layer weights.\n",
    "- We're using the weights of the smallest GPT-2 model via the download_and_load_gpt2(model_size=\"124M\", ...) setting. OpenAI also shares the weights of larger models: 355M, 774M, and 1558M. The overall architecture of these differently sized GPT models is the same, as illustrated in figure 5.17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "20c90e9e-d391-4885-b831-cbac7825882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c0539ce-0555-4abc-bea6-041353f2f7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c7f7d-2576-4049-b811-b537f2e97ce3",
   "metadata": {},
   "source": [
    "![fig5-17](px/fig5-17.png)\n",
    "\n",
    "- After loading the GPT-2 model weights into Python, we still need to transfer them from the\n",
    "settings and params dictionaries into our GPTModel instance. First, we __create a dictionary that lists the differences between the different GPT model sizes__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37663e10-048b-403d-a12a-50a79e3d46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "\"gpt2-small (124M)\":  {\"emb_dim\":  768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "\"gpt2-large (774M)\":  {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "\"gpt2-xl (1558M)\":    {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6bf96-a5f6-42e2-a059-0f145e50b40c",
   "metadata": {},
   "source": [
    "- Suppose we are interested in loading the smallest model, \"gpt2-small (124M)\". We can use __model_configs__ to update our full-length GPT_CONFIG_124M we defined and used earlier throughout the chapter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ef473ba-c8fd-4218-ad93-c5f9257b9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e29ae-1f65-434d-8dcb-6eb7ac13c1b5",
   "metadata": {},
   "source": [
    "- Recall that we used a 256-token length earlier, but the original GPT-2\n",
    "models from OpenAI were trained with a 1,024-token length, so we have to update the\n",
    "NEW_CONFIG accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4b0f946b-1697-4b9d-ae61-898d3400d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d3a48-5682-45c2-bc3f-033d339c9309",
   "metadata": {},
   "source": [
    "- __OpenAI used bias vectors in the multi-head attention module’s linear layers__ to implement the\n",
    "query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore\n",
    "as they don’t improve the modeling performance and are thus unnecessary.\n",
    "- However, since we are working with pretrained weights, __we need to match the settings for consistency and enable these bias vectors__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6b1a09b6-9fac-4cd5-9bd7-1f233536a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c05e67-e293-4d93-b150-e629eb89f522",
   "metadata": {},
   "source": [
    "- Now use the updated NEW_CONFIG dictionary to initialize a new GPTModel instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "94f706da-cef6-4291-813e-52670308205d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da97fb-5fb9-48bd-a619-08ce939c058b",
   "metadata": {},
   "source": [
    "- By default, the GPTModel instance is initialized with random weights for pretraining. The last step to\n",
    "using OpenAI’s model weights is to __override these random weights with the weights we loaded into\n",
    "the params dictionary__.\n",
    "- We will need a utility function that checks whether two tensors or arrays (left and right) have the same dimensions or shape and returns the right tensor as trainable PyTorch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c6b839c7-b403-43e7-99cf-a5cff74759b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef226f-75f2-4c67-bcde-ac2fdece0e99",
   "metadata": {},
   "source": [
    "- define a __load_weights_into_gpt__ function that loads the weights from the params dictionary\n",
    "into a GPTModel instance gpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "13992a95-ea36-49de-81b9-0d9e8a24930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "        \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight  = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79c035-798c-4e2b-bc39-20412e3ba623",
   "metadata": {},
   "source": [
    "- Here we match the weights from OpenAI’s implementation to our GPTModel implementation.\n",
    "- Example: OpenAI stored the weight tensor for the output projection layer for the first transformer block as __params[\"blocks\"][0][\"attn\"][\"c_proj\"][\"w\"]__. In our implementation, this tensor corresponds to\n",
    "__gpt.trf_blocks[b].att.out_proj.weight__, where gpt is a GPTModel instance.\n",
    "- Developing this function took a lot of guesswork - OpenAI used a slightly different naming convention from ours. However, the assign function would alert us if we try to match two tensors with different dimensions. Also, if we made a mistake in this function, we would notice this, as the resulting GPT model would be unable to produce coherent text.\n",
    "- Launch __load_weights_into_gpt__ & load the OpenAI model weights into our GPTModel instance gpt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b02bd16e-443b-449d-ac7f-a53ee004f31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cc57e-71c3-4170-8b61-fa72f8d2629e",
   "metadata": {},
   "source": [
    "- If the model is loaded correctly, we can now use it to generate new text using our previous generate\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b50cd795-807e-467a-81d8-50d2052576ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model          = gpt,\n",
    "    idx            = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens = 25,\n",
    "    context_size   = NEW_CONFIG[\"context_length\"],\n",
    "    top_k          = 50,\n",
    "    temperature    = 1.5)\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01643abd-94db-47bc-8d3d-6c795ee7f38f",
   "metadata": {},
   "source": [
    "# 6 Finetuning for Classication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044704c-a56a-4934-ab69-d69c9f905a69",
   "metadata": {},
   "source": [
    "## 6.1 Different categories of finetuning\n",
    "![fig6-2](px/fig6-2.png)\n",
    "\n",
    "- The most common ways to finetune language models are __instruction-finetuning__ and __classification-\n",
    "finetuning__. Instruction-finetuning involves training a language model on a set of tasks using specific\n",
    "instructions to improve its ability to understand and execute tasks described in natural language\n",
    "prompts.\n",
    "\n",
    "- In __classification-finetuning__, the model is trained to recognize a specific set of class labels, such as \"spam\" and \"not spam.\" classification-finetuned models are __restricted to predicting classes it has encountered during its training__ — but it can't say anything else about the input text.\n",
    "\n",
    "- an __instruction-finetuned__ model typically can undertake a broader range of tasks. We can view a\n",
    "classification-finetuned model as highly specialized, and generally, it is easier to develop a\n",
    "specialized model than a generalist model that works well across various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930ebc1-286b-4baa-a5d9-99063deed0f5",
   "metadata": {},
   "source": [
    "## 6.2 Preparing the dataset\n",
    "- To provide an intuitive and useful example of classification-finetuning, we will work with a text\n",
    "message dataset with spam and non-spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ba02f6c3-57b8-4ae3-bd97-0975f70af752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url            = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path       = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e43241df-c996-4a12-b9a8-b99b9c3ec585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load into pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "88b303dd-b684-425c-8369-7a086bd4e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# review the class label distribution\n",
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809a632-011e-4e19-b54f-ba028bff0774",
   "metadata": {},
   "source": [
    "- We will undersample the dataset to include 747 instances from each class for simplicity. While there are several other methods to handle class imbalances, these are beyond the scope of a book on large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5aac4eb0-bdc6-425d-84f6-be2fdbd7d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam    = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    ham_subset  = df[df[\"Label\"] == \"ham\"].sample(\n",
    "        num_spam, \n",
    "        random_state=123)\n",
    "    balanced_df = pd.concat(\n",
    "        [ham_subset, \n",
    "         df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bdcbe332-1e09-4ce7-8d2b-895933082522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels to integers (0 and 1)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbde4cb-20b1-42cb-9333-223df846e58e",
   "metadata": {},
   "source": [
    "- This is similar to converting text into token IDs. However, instead of using the GPT vocabulary, which consists of more than 50,000 words, we are dealing with just two token IDs: 0 and 1.\n",
    "- Create a random_split function to split the dataset into three parts: 70% for training, 10% for validation, and 20% for testing. (These ratios are common in machine learning to train, adjust, and evaluate models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c3b4709e-ebc7-42e2-86c6-3eb865c306a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df             = df.sample(frac=1, random_state=123).reset_index(drop=True) #A\n",
    "    train_end      = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    \n",
    "    train_df      = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df       = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1) #D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e0b4b71f-b7ac-4871-a887-f37f00e3bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset as CSV for later reuse\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b3ba2-24c7-46ef-9b64-2b7a6466ab55",
   "metadata": {},
   "source": [
    "## 6.3 Creating data loaders\n",
    "- Previously we used a sliding window to generate uniformly sized text\n",
    "chunks, which were then grouped into batches for more efficient model training. Each chunk functioned as an individual training instance.\n",
    "- Now we are working with a spam dataset that contains text messages of\n",
    "varying lengths. To batch these messages as we did with the text chunks, we have two options:\n",
    "    1.__Truncate__ all messages to the length of the shortest message in the dataset or batch.\n",
    "    2. __Pad__ all messages to the length of the longest message in the dataset or batch.\n",
    "- Option 1 is computationally cheaper, but it may result in significant information loss if shorter messages are much smaller than the average or longest messages, potentially reducing model performance. So, we opt for the second option, which preserves the entire content of all messages.\n",
    "\n",
    "- To implement option 2, we __add padding tokens__ (\"<|endoftext|>\") to all shorter messages. Instead of appending the string \"<|endoftext|>\" to each of the text messages, we can add the corresponding token ID.\n",
    "\n",
    "- We assume 50,256 is the token ID of the padding token \"<|endoftext|>\". We can double-check that this by encoding the \"<|endoftext|>\" using the\n",
    "GPT-2 tokenizer from the tiktoken package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2ea7c0cd-52f5-4400-92a6-4e6fbe6fc826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbd8cf-fe63-448b-a864-a520e4016d60",
   "metadata": {},
   "source": [
    "- We need to design a __PyTorch Dataset__, which specifies how the data is loaded and processed, before we can instantiate the data loaders.\n",
    "- Define a __SpamDataset__ class to handle several key tasks:\n",
    "    - identify the longest sequence in the training dataset\n",
    "    - encode the text messages\n",
    "    - ensure all other sequences are padded with a padding token to match the length of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d0edcb89-0778-49db-bf13-5e96c5c7a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts]\n",
    "\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450a313-0a88-492d-95be-e3d69b9b51de",
   "metadata": {},
   "source": [
    "- __SpamDataset__ loads data from the CSV files we created earlier, tokenizes the text using the GPT-2 tokenizer from tiktoken and allows us to pad or truncate the sequences to a uniform length determined by either the longest sequence or a predefined maximum length. This ensures each\n",
    "input tensor is of the same size, which is necessary to create the batches in the training data loader we implement next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6c19b026-fc41-427f-a30e-4015434d01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5cbd9-26ab-4bc7-8379-4c62099049e1",
   "metadata": {},
   "source": [
    "- The longest sequence length is stored in the __max_length__ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "494afd1a-c354-45d9-96ed-34fd34068038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d9264-de64-4d1a-9e01-6497f5795980",
   "metadata": {},
   "source": [
    "- 120 tokens is a common length for text messages. The model can handle sequences of up to 1,024 tokens, given its context length limit. If your dataset includes longer texts, you can pass __max_length=1024__ when creating the training dataset.\n",
    "- Next: __pad the validation and test sets__ to match the length of the longest training sequence. Any validation and test set samples exceeding the length of the longest training example are truncated using encoded_text[:self.max_length] in the SpamDataset code we defined earlier. This truncation is optional; you could also set max_length=None for both validation\n",
    "and test sets, provided there are no sequences exceeding 1,024 tokens in these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dba52989-8ec3-41e0-8f00-3f976b7bb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab017289-d4ff-4416-bee9-cda2a143caed",
   "metadata": {},
   "source": [
    "- Using the datasets as inputs, we can instantiate the data loaders similarly to what we did in chapter\n",
    "2. However, in this case, the targets represent class labels rather than the next tokens in the text.\n",
    "For instance, choosing a batch size of 8, each batch will consist of 8 training examples of length 120\n",
    "and the corresponding class label of each example, as illustrated in figure 6.7.\n",
    "\n",
    "![fig6-7](px/fig6-7.png)\n",
    "\n",
    "- Next: Create the training, validation, and test set data loaders that load the text messages and labels in batches of size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0f82cef4-923c-47eb-a245-11c5da462a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,   batch_size=batch_size,               num_workers=num_workers, drop_last=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,  batch_size=batch_size,               num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b504c-8e4f-4d31-a791-81fbe4a3af93",
   "metadata": {},
   "source": [
    "- Verify the data loaders are working & returning batches of the expected size - iterate over the training loader and then print the tensor dimensions of the last batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c983d131-065e-47ea-b8bb-8d434ad446d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "print(\"Input batch dimensions:\", torch.Size([8, 120]))\n",
    "print(\"Label batch dimensions:\", torch.Size([8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5c0d3-72cd-4d7c-9eed-5013f4743708",
   "metadata": {},
   "source": [
    "- The input batches consist of 8 training examples with 120 tokens each, as expected. The label tensor stores the class labels corresponding to the 8 training examples.\n",
    "- Verify the dataset size using the total number of batches in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "68442a26-108e-400f-9fe6-164b50ce178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7ebb9-4933-422e-ae11-1d0d46c74726",
   "metadata": {},
   "source": [
    "## 6.4 Initializing a model with pretrained weights\n",
    "\n",
    "![fig6-8](px/fig6-8.png)\n",
    "\n",
    "- Reuse the configurations from chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "17fd49f5-b098-4bc9-ba00-ccfd73696611",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,# Vocabulary size\n",
    "    \"context_length\": 1024,# Context length\n",
    "    \"drop_rate\": 0.0,# Dropout rate\n",
    "    \"qkv_bias\": True# Query-key-value bias\n",
    "    }\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96840d-c91b-4ac4-872c-ac5f8de49179",
   "metadata": {},
   "source": [
    "- import __download_and_load_gpt2__ from __gpt_download.py__; reuse the __GPTModel__ class and\n",
    "__load_weights_into_gpt__ from chapter 5 to load the downloaded weights into the GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2671fe4e-b292-4bf7-aee1-778126430a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "#from chapter05 import GPTModel, load_weights_into_gpt\n",
    "\n",
    "model_size       = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "model            = GPTModel(BASE_CONFIG)\n",
    "\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5cd1fe-b69d-41f8-9716-2aab6d09f417",
   "metadata": {},
   "source": [
    "- After loading the model weights into GPTModel, use the text generation utility function to ensure that the model generates coherent text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8550e82a-ceeb-46f4-8193-ef62fef5aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "#from chapter04 import generate_text_simple\n",
    "#from chapter05 import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model          = model,\n",
    "    idx            = text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens = 15,\n",
    "    context_size   = BASE_CONFIG[\"context_length\"])\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08180f35-b0c7-406d-8231-be86a23cf9ca",
   "metadata": {},
   "source": [
    "- Before we start finetuning the model as a spam classifier, let's see if the model can already classify spam messages by prompting it with instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "248090c2-485b-4923-855a-b55dca4367de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "\"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "\" 'You are a winner you have been specially\"\n",
    "\" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "token_ids = generate_text_simple(\n",
    "    model          = model,\n",
    "    idx            = text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens = 23,\n",
    "    context_size   = BASE_CONFIG[\"context_length\"])\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61706f14-96f8-40fb-8227-b7afcff098a5",
   "metadata": {},
   "source": [
    "- The model still struggles with following instructions. This isn't a surprise - it has undergone only pretraining and lacks instruction-finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c271190-baa7-405c-9b27-6dff9691dda8",
   "metadata": {},
   "source": [
    "## 6.5 Adding a classification head\n",
    "\n",
    "- In this section, we modify the pretrained LLM to prepare it for classification-finetuning by replacing the original output layer, which maps the hidden representation to a vocabulary of 50,257, with a __smaller output layer that maps to two classes__: 0 (\"not spam\") and 1 (\"spam\").\n",
    "\n",
    "![fig6-9](px/fig6-9.png)\n",
    "\n",
    "### OUTPUT LAYER NODES\n",
    "- We __could technically use a single output node__ since we are dealing with a binary classification task. However, this __would require modifying the loss function__. (see Reference section in appendix B.)\n",
    "- Therefore, we choose a more general approach where the number of output nodes matches the number of classes. For example,\n",
    "for a 3-class problem, such as classifying news articles as \"Technology\", \"Sports\", or \"Politics\", we would use three output nodes, and so forth.\n",
    "\n",
    "- First, let's print the model architecture via __print(model)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5a616873-5292-4ebf-95d3-73d41045a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c4b71-4455-4d21-9969-cc3cdbbd6fbc",
   "metadata": {},
   "source": [
    "### FINETUNING SELECTED LAYERS VERSUS ALL LAYERS\n",
    "- Since we start with a pretrained model, __it's not necessary to finetune all model layers__. The lower layers generally capture basic language structures and semantics that are applicable across a wide range of tasks and\n",
    "datasets. Finetuning only the last layers (layers near the output), which are more specific task-specific features, can often be sufficient.\n",
    "- A __nice side effect__ is that it is computationally more efficient to finetune only a small number of layers. See more info in the __References section in appendix B__.\n",
    "\n",
    "- To get the model ready for classification-finetuning, we __first freeze the model__, (making all layers non-trainable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c1197ac4-fa10-4755-96f2-ba96398db3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57482371-440a-4e33-a738-88363d1f274f",
   "metadata": {},
   "source": [
    "- replace the output layer (__model.out_head__), which originally maps the layer inputs to 50,257 dimensions (the size of the vocabulary).\n",
    "- Note: we use BASE_CONFIG[\"emb_dim\"] (768 in the \"gpt2-small (124M)\" model) to keep the code more general. This means we can also use the same code to work with the larger GPT-2 model variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f4a4ebc9-f50c-4fcb-884b-3fcddedfee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "in_features=BASE_CONFIG[\"emb_dim\"],\n",
    "out_features=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e327df-3c8c-4453-9304-9b745720e8e8",
   "metadata": {},
   "source": [
    "- This __model.out_head__ output layer has __requires_grad attribute set to True by default__ - so it's the only layer that will be updated during training.\n",
    "- Technically, training the output layer we just added is sufficient. However, as I found in experiments, __finetuning additional layers can noticeably improve the predictive performance__ of the finetuned model. (See References in appendix C.)\n",
    "- Also: __configure the last transformer block and the final LayerNorm module, which connects this block to the output layer, to be trainable__.\n",
    "\n",
    "![fig6-10](px/fig6-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "81c33471-f41b-4ab6-b661-27b6e659bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763aae16-d5a3-499c-9319-a8363cee3f6d",
   "metadata": {},
   "source": [
    "- Even though we added a new output layer and marked certain layers as trainable or non-trainable, we can still use this model in a similar way to previous chapters. The code encodes the inputs into a tensor of 4 input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "999145c8-9b65-432d-80a0-23a86e5d0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a374ad-38d1-4c19-af29-4395296963e7",
   "metadata": {},
   "source": [
    "- Pass the encoded token IDs to the model as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fb8dada5-98bf-4dbf-af96-9d51d361edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee9ea4-0954-4e75-953e-4b0f4460d90a",
   "metadata": {},
   "source": [
    "- Previously a similar input would have produced an output tensor of [1, 4, 50257], where 50,257 represents the vocabulary size.\n",
    "- The number of output rows corresponds to the number of input tokens (in this case, 4).\n",
    "- However, each output's embedding dimension (the number of columns) is 2 instead of 50,257 since we replaced the output layer.\n",
    "- Remember we want to finetune this model so that it returns a spam/not spam class label. We don't need to finetune all\n",
    "4 output rows but can focus on a single output token.\n",
    "- Extract the last output token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cbc3c1b9-1025-4c22-b520-5a2656e6b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc039eff-c61b-4459-858a-8627f8f10ced",
   "metadata": {},
   "source": [
    "- Why we are interested in the last output token, and not the 1st, 2nd, or 3rd output tokens?\n",
    "- Recall how attention mechanism establishes a relationship between each input token and every other input token. We then introduced the __causal attention mask__, commonly used in GPT-like models. It restricts a token's focus to only its current position and those before it, ensuring that each token can only be influenced by itself and preceding tokens.\n",
    "\n",
    "![fig6-12](px/fig6-12.png)\n",
    "\n",
    "- Given this setup, the last token in a sequence accumulates the most information since __it is the only token with access to data from all the previous tokens__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcca80-61ff-4170-b149-452f80e15668",
   "metadata": {},
   "source": [
    "## 6.6 Calculating the classification loss and accuracy\n",
    "- We previously computed the token ID of the next token generated by the LLM by converting the 50,257 outputs into probabilities via __softmax__, then returning the position of the highest probability via __argmax__. We'll take the same approach to calculate whether the model outputs a \"spam\" or \"not spam\" prediction using 2-dimensional outputs instead of 50,257-dimensional outputs.\n",
    "- Consider the last token output from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "02513f11-84a0-478b-a602-f312b05cbabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2331cf0-670f-49a0-b8f8-544204c7e89d",
   "metadata": {},
   "source": [
    "- Corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1061954b-d65d-4564-8b65-08aaa1932cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1887b5-ed28-4ffd-91f2-d9913b5e7086",
   "metadata": {},
   "source": [
    "- 1 indicates the model predicts that the input text is \"spam.\" Softmax is optional because the largest outputs directly correspond to the highest probability scores. Hence, we can simplify the code as follows, without softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "79f57625-947c-4f22-94fd-d504bb0028fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a384e-a074-484b-a6ef-3a4b9027da0a",
   "metadata": {},
   "source": [
    "- To find __classification accuracy__ (the pct of correct predictions across a dataset), we apply the argmax-based prediction code to all examples in the dataset and calculate the proportion of correct predictions with a __calc_accuracy_loader__ utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c6f04feb-255b-46b2-a1d4-ef253cd1f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :] # logits of the last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples        += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64e92a-26b6-497e-a3a2-99da5c9e9f61",
   "metadata": {},
   "source": [
    "- use the utility to find classification accuracies across various datasets estimated from 10 batches for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a5d3cf8e-5208-433c-9ed5-9ea453d0a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy   = calc_accuracy_loader(  val_loader, model, device, num_batches=10)\n",
    "test_accuracy  = calc_accuracy_loader( test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a37a4-932a-4e98-b254-9df1d73f0d76",
   "metadata": {},
   "source": [
    "- The prediction accuracies are near-random (50% in this case).\n",
    "- Before finetuning we need to define the loss function that we will optimize during training. Our objective is to maximize the spam classification accuracy, which means that the preceding code should output correct class labels: 0 for non-spam and 1 for spam texts.\n",
    "- __Classification accuracy is not a differentiable function__, so we use __cross entropy loss__ as a proxy to maximize accuracy. The __calc_loss_batch__ function remains the same as in chapter 5, with one adjustment: we focus on optimizing only the last token, model(input_batch)[:, -1, :], rather than all tokens model(input_batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6068a35a-36f1-43ee-9eb5-30b1d59ad8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits                    = model(input_batch)[:, -1, :]\n",
    "    loss                      = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437b409-f0ae-49ea-8f70-45189ccd04d7",
   "metadata": {},
   "source": [
    "- __calc_loss_batch__ finds the loss for a single batch obtained from the data loaders.\n",
    "- __calc_loss_loader__ finds the loss for all batches in a data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "930210c7-7fa0-4f70-894a-ed85a1d1355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.194\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss        = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# compute the initial loss for each data set:\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss   = calc_loss_loader(  val_loader, model, device, num_batches=5)\n",
    "    test_loss  = calc_loss_loader( test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97588929-2dfe-498c-9952-5bec3fd6160c",
   "metadata": {},
   "source": [
    "## 6.7 Finetuning the model on supervised data\n",
    "- The training loop is the same as in chapter 5. The difference being that we find the __classification\n",
    "accuracy__ instead of generating a sample text for evaluating the model.\n",
    "- Track the number of training examples instead of the number of tokens. Find the accuracy after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "188c73e8-3cf1-4bd3-8f75-48e0f8189c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs, eval_freq, eval_iter, tokenizer):\n",
    "    \n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step                     = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "    \n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy   = calc_accuracy_loader(  val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef93303-045c-4fcd-a55f-1e13768c9c9e",
   "metadata": {},
   "source": [
    "- __evaluate_model__ is identical to that used in chap 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b6d4ab3c-6466-4937-94f4-0cacb6f649a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss   = calc_loss_loader(  val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3bd63-3b38-4db9-bfdb-4464bd8d4d1b",
   "metadata": {},
   "source": [
    "- Initialize the optimizer, set the number of training epochs, and start training with __train_classifier_simple__.\n",
    "- The training takes about 6 minutes on an M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bb3c0393-4d85-4aeb-a25f-2564441f40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
      "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
      "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
      "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
      "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
      "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
      "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
      "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
      "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
      "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 11.49 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=50, eval_iter=5, tokenizer=tokenizer)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d95a53-feec-4e16-a9a5-303f1c9e454f",
   "metadata": {},
   "source": [
    "- plot the loss function for the training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "42f14b03-0243-4738-ae56-e89c6c0bccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABXgElEQVR4nO3deVxU9f748dfMwAz7viPiBriCu7lTkmJl2erX6y0ty1thZWbbrdTsV7TYzUqzspvcupWlpXXLJcR9X1FwwR1Q2VxYhQFmzu+PgdFJXEBgBnw/H4/zYM7nfM457/lEvjmf8znno1IURUEIIYQQNklt7QCEEEIIcWWSqIUQQggbJolaCCGEsGGSqIUQQggbJolaCCGEsGGSqIUQQggbJolaCCGEsGGSqIUQQggbJolaCCGEsGGSqIUQ1yU6OppJkyZZOwwhbjqSqIVoJOPGjUOlUl22xMbGWjs0IYQNs7N2AELcTGJjY5k/f75FmU6ns1I0QoimQK6ohWhEOp2OgIAAi8XT0xOANWvWoNVqWb9+vbn++++/j5+fHzk5OQAsX76cAQMG4OHhgbe3N3fddRdHjx411z9x4gQqlYqffvqJgQMH4ujoSK9evTh06BDbt2+nZ8+euLi4MHz4cPLy8sz7jRs3jpEjR/Lmm2/i6+uLm5sbTz75JOXl5Vf8Lnq9nilTphAcHIyzszN9+vRhzZo15u3p6emMGDECT09PnJ2d6dSpE0uXLr3i8T777DPCwsJwcHDA39+fBx54wLzNaDQSHx9P69atcXR0JCoqikWLFlnsn5qayvDhw3FxccHf35+HH36YM2fOmLdHR0fz7LPP8tJLL+Hl5UVAQADTp0+/YjxC2ApJ1ELYiOp7wA8//DAFBQXs3r2bN954g6+++gp/f38ASkpKmDx5Mjt27CApKQm1Ws29996L0Wi0ONa0adN4/fXX2bVrF3Z2dvztb3/jpZde4uOPP2b9+vUcOXKEqVOnWuyTlJTEgQMHWLNmDT/88AO//PILb7755hXjnThxIps3b2bBggXs3buXBx98kNjYWA4fPgxAXFwcer2edevWkZKSwnvvvYeLi0uNx9qxYwfPPvssM2bMIC0tjeXLlzNo0CDz9vj4eL755hs+//xz9u3bx/PPP8/f//531q5dC0B+fj633XYb3bp1Y8eOHSxfvpycnBweeughi/P85z//wdnZma1bt/L+++8zY8YMEhMTr/O/kBBWogghGsXYsWMVjUajODs7Wyxvv/22uY5er1e6du2qPPTQQ0rHjh2VJ5544qrHzMvLUwAlJSVFURRFOX78uAIoX331lbnODz/8oABKUlKSuSw+Pl6JiIiwiM3Ly0spKSkxl82dO1dxcXFRDAaDoiiKMnjwYOW5555TFEVR0tPTFY1Go5w6dcoiniFDhiivvvqqoiiK0qVLF2X69OnX1TY///yz4ubmphQWFl62raysTHFyclI2bdpkUT5+/Hhl9OjRiqIoyltvvaUMHTrUYntmZqYCKGlpaeb4BwwYYFGnV69eyssvv3xdMQphLXKPWohGdOuttzJ37lyLMi8vL/NnrVbLd999R2RkJKGhoXz00UcWdQ8fPszUqVPZunUrZ86cMV9JZ2Rk0LlzZ3O9yMhI8+fqq/EuXbpYlOXm5locOyoqCicnJ/N63759KS4uJjMzk9DQUIu6KSkpGAwGwsPDLcr1ej3e3t4APPvsszz11FP8+eefxMTEcP/991vEdanbb7+d0NBQ2rRpQ2xsLLGxsdx77704OTlx5MgRLly4wO23326xT3l5Od26dQNgz549rF69usYr9qNHj5rj/Ov5AwMDL2sHIWyNJGohGpGzszPt2rW7ap1NmzYBcO7cOc6dO4ezs7N524gRIwgNDWXevHkEBQVhNBrp3LnzZfeS7e3tzZ9VKlWNZX/tLq+N4uJiNBoNO3fuRKPRWGyrTpaPP/44w4YN448//uDPP/8kPj6eDz/8kGeeeeay47m6urJr1y7WrFnDn3/+ydSpU5k+fTrbt2+nuLgYgD/++IPg4GCL/aoH4hUXFzNixAjee++9y44dGBho/nxpG8CNt4MQjUEStRA25OjRozz//PPMmzePH3/8kbFjx7Jy5UrUajVnz54lLS2NefPmMXDgQAA2bNhQb+fes2cPpaWlODo6ArBlyxZcXFwICQm5rG63bt0wGAzk5uaaY6lJSEgITz75JE8++SSvvvoq8+bNqzFRA9jZ2RETE0NMTAzTpk3Dw8ODVatWcfvtt6PT6cjIyGDw4ME17tu9e3d+/vlnWrVqhZ2d/LMmmhf5jRaiEen1erKzsy3K7Ozs8PHxwWAw8Pe//51hw4bx6KOPEhsbS5cuXfjwww958cUX8fT0xNvbmy+//JLAwEAyMjJ45ZVX6i228vJyxo8fz+uvv86JEyeYNm0aEydORK2+fMxpeHg4Y8aM4ZFHHuHDDz+kW7du5OXlkZSURGRkJHfeeSeTJk1i+PDhhIeHc/78eVavXk2HDh1qPPfvv//OsWPHGDRoEJ6enixduhSj0UhERASurq5MmTKF559/HqPRyIABAygoKGDjxo24ubkxduxY4uLimDdvHqNHjzaP6j5y5AgLFizgq6++uuyqX4imRBK1EI1o+fLlFl2xABERERw8eJC3336b9PR0fv/9d8DUZfvll18yevRohg4dSlRUFAsWLODZZ5+lc+fORERE8MknnxAdHV0vsQ0ZMoSwsDAGDRqEXq9n9OjRV318af78+fy///f/eOGFFzh16hQ+Pj7ccsst3HXXXQAYDAbi4uI4efIkbm5uxMbGXnbPvZqHhwe//PIL06dPp6ysjLCwMH744Qc6deoEwFtvvYWvry/x8fEcO3YMDw8Punfvzj//+U8AgoKC2LhxIy+//DJDhw5Fr9cTGhpKbGxsjX9oCNGUqBRFUawdhBDCusaNG0d+fj5LliyxdihCiL+QPzWFEEIIGyaJWgghhLBh0vUthBBC2DC5ohZCCCFsmCRqIYQQwoZJohZCCCFsmCTqGzBnzhxatWqFg4MDffr0Ydu2bdYOqcGsW7eOESNGEBQUhEqluuwxHkVRmDp1KoGBgTg6OhITE2OeRanauXPnGDNmDG5ubnh4eDB+/Hjz6yGr7d27l4EDB+Lg4EBISAjvv/9+Q3+1ehEfH0+vXr1wdXXFz8+PkSNHkpaWZlGnrKyMuLg4vL29cXFx4f777zdPX1ktIyODO++8EycnJ/z8/HjxxReprKy0qLNmzRq6d++OTqejXbt2JCQkNPTXqxdz584lMjISNzc33Nzc6Nu3L8uWLTNvv9nbpybvvvsuKpWKSZMmmcuknWD69OmoVCqLpX379ubtza6NrDolSBO2YMECRavVKl9//bWyb98+5YknnlA8PDyUnJwca4fWIJYuXaq89tpryi+//KIAyuLFiy22v/vuu4q7u7uyZMkSZc+ePcrdd9+ttG7dWiktLTXXiY2NVaKiopQtW7Yo69evV9q1a2ee/UhRFKWgoEDx9/dXxowZo6Smpio//PCD4ujoqHzxxReN9TXrbNiwYcr8+fOV1NRUJTk5WbnjjjuUli1bKsXFxeY6Tz75pBISEqIkJSUpO3bsUG655RalX79+5u2VlZVK586dlZiYGGX37t3K0qVLFR8fH/NsVIqiKMeOHVOcnJyUyZMnK/v371c+/fRTRaPRKMuXL2/U71sXv/32m/LHH38ohw4dUtLS0pR//vOfir29vZKamqooirTPX23btk1p1aqVEhkZaZ61TFGknRRFUaZNm6Z06tRJycrKMi95eXnm7c2tjSRR11Hv3r2VuLg487rBYFCCgoKU+Ph4K0bVOP6aqI1GoxIQEKB88MEH5rL8/HxFp9MpP/zwg6IoirJ//34FULZv326us2zZMkWlUpmnSvzss88UT09PRa/Xm+u8/PLLFtMxNhW5ubkKoKxdu1ZRFFN72NvbKwsXLjTXOXDggAIomzdvVhTF9MeQWq1WsrOzzXXmzp2ruLm5mdvkpZdeUjp16mRxrlGjRinDhg1r6K/UIDw9PZWvvvpK2ucvioqKlLCwMCUxMdFielFpJ5Np06YpUVFRNW5rjm0kXd91UF5ezs6dO4mJiTGXqdVqYmJi2Lx5sxUjs47jx4+TnZ1t0R7u7u706dPH3B6bN2/Gw8ODnj17muvExMSgVqvZunWruc6gQYPQarXmOsOGDSMtLY3z58830repHwUFBcDFKSx37txJRUWFRRu1b9+eli1bWrRRly5dzNNSgun7FxYWsm/fPnOdS49RXaep/d4ZDAYWLFhASUkJffv2lfb5i7i4OO68887Lvou000WHDx8mKCiINm3aMGbMGDIyMoDm2UaSqOvgzJkzGAwGi//IYJrj968TLtwMqr/z1dojOzsbPz8/i+12dnZ4eXlZ1KnpGJeeoykwGo1MmjSJ/v37m+eIzs7ORqvV4uHhYVH3r210re9/pTqFhYWUlpY2xNepVykpKbi4uKDT6XjyySdZvHgxHTt2lPa5xIIFC9i1axfx8fGXbZN2MunTpw8JCQksX76cuXPncvz4cQYOHEhRUVGzbCOZlEOIehYXF0dqamq9TkHZXERERJCcnExBQQGLFi1i7NixrF271tph2YzMzEyee+45EhMTcXBwsHY4Nmv48OHmz5GRkfTp04fQ0FB++ukn8zStzYlcUdeBj48PGo3mslGEOTk5BAQEWCkq66n+zldrj4CAAHJzcy22V1ZWcu7cOYs6NR3j0nPYuokTJ/L777+zevVqWrRoYS4PCAigvLyc/Px8i/p/baNrff8r1XFzc2sS/0BptVratWtHjx49iI+PJyoqio8//ljap8rOnTvJzc2le/fu2NnZYWdnx9q1a/nkk0+ws7PD399f2qkGHh4ehIeHc+TIkWb5uySJug60Wi09evQgKSnJXGY0GklKSqJv375WjMw6WrduTUBAgEV7FBYWsnXrVnN79O3bl/z8fHbu3Gmus2rVKoxGI3369DHXWbduHRUVFeY6iYmJRERE4Onp2Ujfpm4URWHixIksXryYVatW0bp1a4vtPXr0wN7e3qKN0tLSyMjIsGijlJQUiz9oEhMTcXNzo2PHjuY6lx6juk5T/b0zGo3o9XppnypDhgwhJSWF5ORk89KzZ0/GjBlj/iztdLni4mKOHj1KYGBg8/xdavTha83EggULFJ1OpyQkJCj79+9XJkyYoHh4eFiMImxOioqKlN27dyu7d+9WAOVf//qXsnv3biU9PV1RFNPjWR4eHsqvv/6q7N27V7nnnntqfDyrW7duytatW5UNGzYoYWFhFo9n5efnK/7+/srDDz+spKamKgsWLFCcnJyaxONZTz31lOLu7q6sWbPG4pGRCxcumOs8+eSTSsuWLZVVq1YpO3bsUPr27av07dvXvL36kZGhQ4cqycnJyvLlyxVfX98aHxl58cUXlQMHDihz5sxpMo/VvPLKK8ratWuV48ePK3v37lVeeeUVRaVSKX/++aeiKNI+V3LpqG9FkXZSFEV54YUXlDVr1ijHjx9XNm7cqMTExCg+Pj5Kbm6uoijNr40kUd+ATz/9VGnZsqWi1WqV3r17K1u2bLF2SA1m9erVCnDZMnbsWEVRTI9ovfHGG4q/v7+i0+mUIUOGKGlpaRbHOHv2rDJ69GjFxcVFcXNzUx599FGlqKjIos6ePXuUAQMGKDqdTgkODlbefffdxvqKN6SmtgGU+fPnm+uUlpYqTz/9tOLp6ak4OTkp9957r5KVlWVxnBMnTijDhw9XHB0dFR8fH+WFF15QKioqLOqsXr1a6dq1q6LVapU2bdpYnMOWPfbYY0poaKii1WoVX19fZciQIeYkrSjSPlfy10Qt7WR6TCowMFDRarVKcHCwMmrUKOXIkSPm7c2tjWT2LCGEEMKGyT1qIYQQwoZJohZCCCFsmCRqIYQQwoZJohZCCCFsmCRqIYQQwoZJohZCCCFsmCTqG6DX65k+fTp6vd7aodg0aadrkza6Nmmja5M2uram2EZWfY46Pj6eX375hYMHD+Lo6Ei/fv147733iIiIuOI+CQkJPProoxZlOp2OsrKyhg73MoWFhbi7u1NQUICbm1ujn7+pkHa6Nmmja5M2ujZpo2trim1k1SvqtWvXEhcXx5YtW0hMTKSiooKhQ4dSUlJy1f3c3NzIysoyL+np6Y0UsRBCCNG4rDrN5fLlyy3WExIS8PPzY+fOnQwaNOiK+6lUqiYzm5IQQghxI2xqPuqCggIAvLy8rlqvuLiY0NBQjEYj3bt355133qFTp07XdY7Kykp2796Nv78/avWNdSgUFRUBcOrUKQoLC2/oWM2ZtNO1SRtdm7TRtUkbXZuttJHRaCQnJ4du3bphZ3f1VGwz7/o2Go3cfffd5Ofns2HDhivW27x5M4cPHyYyMpKCggJmzpzJunXr2Ldvn8X8v9X0er3FoIGdO3dy2223Nch3EEIIIWpj27Zt9OrV66p1bCZRP/XUUyxbtowNGzbUmHCvpKKigg4dOjB69Gjeeuuty7ZPnz6dN99887Lybdu2ERgYeEMxCyGEEHWRlZVF7969SU9Pp2XLlletaxOJeuLEifz666+sW7eO1q1b13r/Bx98EDs7O3744YfLtv31ivrUqVN07NiRzMzMWv1BIIQQQtSXkydPEhIScl25yKqjvhVFYeLEiSxevJhVq1bVKUkbDAZSUlKueHWs0+lwc3MzL66urjcathBCCNForDqYLC4uju+//55ff/0VV1dXsrOzAXB3d8fR0RGARx55hODgYOLj4wGYMWMGt9xyC+3atSM/P58PPviA9PR0Hn/8cat9DyGEEKKhWDVRz507F4Do6GiL8vnz5zNu3DgAMjIyLEZnnz9/nieeeILs7Gw8PT3p0aMHmzZtomPHjo0VthBCCNFobOIedWOqzX0BIcTNx2AwUFFRYe0wRBNnb2+PRqO54vba5CKbeo5aCCGsRVEUsrOzyc/Pt3Yoopnw8PAgICAAlUp1Q8eRRH0jSvMhYwu4t4CAztaORghxA6qTtJ+fH05OTjf8j6u4eSmKwoULF8jNzQW44UeBJVHfiFX/D7bPgz5PwvD3rB2NEKKODAaDOUl7e3tbOxzRDFQPiM7NzcXPz++q3eDXItNc3ohW/U0/T2y0bhxCiBtSfU/aycnJypGI5qT69+lGxzxIor4RoVWJOicVLpyzbixCiBsm3d2iPtXX75Mk6hvh4gc+4YACGZutHY0QQohmSBL1jWo1wPRTur+FEM1Eq1atmDVr1nXXX7NmDSqVqsFHzCckJODh4dGg57BFkqhvVHX394n11o1DCHHTUalUV12mT59ep+Nu376dCRMmXHf9fv36kZWVhbu7e53OJ65ORn3fqOor6uwU0+Najh7WjEYIcRPJysoyf/7xxx+ZOnUqaWlp5jIXFxfzZ0VRMBgM15z7GMDX17dWcWi1WgICAmq1j7h+ckV9o1wDwLsdpvvUW6wdjRDiJhIQEGBe3N3dUalU5vWDBw/i6urKsmXL6NGjBzqdjg0bNnD06FHuuece/P39cXFxoVevXqxcudLiuH/t+lapVHz11Vfce++9ODk5ERYWxm+//Wbe/teu7+ou6hUrVtChQwdcXFyIjY21+MOisrKSZ599Fg8PD7y9vXn55ZcZO3YsI0eOrFUbzJ07l7Zt26LVaomIiODbb781b1MUhenTp9OyZUt0Oh1BQUE8++yz5u2fffYZYWFhODg44O/vzwMPPFCrczcWSdT1Qbq/hWh2FEXhQnmlVZb6fLPzK6+8wrvvvsuBAweIjIykuLiYO+64g6SkJHbv3k1sbCwjRowgIyPjqsd58803eeihh9i7dy933HEHY8aM4dy5Kz/tcuHCBWbOnMm3337LunXryMjIYMqUKebt7733Ht999x3z589n48aNFBYWsmTJklp9t8WLF/Pcc8/xwgsvkJqayj/+8Q8effRRVq9eDcDPP//MRx99xBdffMHhw4dZsmQJXbp0AWDHjh08++yzzJgxg7S0NJYvX86gQYNqdf7GIl3f9aHVANj1H0iXAWVCNBelFQY6Tl1hlXPvnzEMJ239/PM8Y8YMbr/9dvO6l5cXUVFR5vW33nqLxYsX89tvvzFx4sQrHmfcuHGMHj0agHfeeYdPPvmEbdu2ERsbW2P9iooKPv/8c9q2bQvAxIkTmTFjhnn7p59+yquvvsq9994LwOzZs1m6dGmtvtvMmTMZN24cTz/9NACTJ09my5YtzJw5k1tvvZWMjAwCAgKIiYnB3t6eli1b0rt3b8A04ZOzszN33XUXrq6uhIaG0q1bt1qdv7HIFXV9qL6iztoDZQXWjUUIIS7Rs2dPi/Xi4mKmTJlChw4d8PDwwMXFhQMHDlzzijoyMtL82dnZGTc3N/MrMmvi5ORkTtJgeo1mdf2CggJycnLMSRNAo9HQo0ePWn23AwcO0L9/f4uy/v37c+DAAQAefPBBSktLadOmDU888QSLFy+msrISgNtvv53Q0FDatGnDww8/zHfffceFCxdqdf7GIlfU9cE9GDxbw/njkLEVwodaOyIhxA1ytNewf8Ywq527vjg7O1usT5kyhcTERGbOnEm7du1wdHTkgQceoLy8/KrHsbe3t1hXqVQYjcZa1W/syRpDQkJIS0tj5cqVJCYm8vTTT/PBBx+wdu1aXF1d2bVrF2vWrOHPP/9k6tSpTJ8+ne3bt9vcI2ByRV1fIu6A8FjQOl+7rhDC5qlUKpy0dlZZGvINaRs3bmTcuHHce++9dOnShYCAAE6cONFg56uJu7s7/v7+bN++3VxmMBjYtWtXrY7ToUMHNm60vOW4ceNGOnbsaF53dHRkxIgRfPLJJ6xZs4bNmzeTkpICgJ2dHTExMbz//vvs3buXEydOsGrVqhv4Zg1DrqjrS+w71o5ACCGuKSwsjF9++YURI0agUql44403rnpl3FCeeeYZ4uPjadeuHe3bt+fTTz/l/Pnztfoj5cUXX+Shhx6iW7duxMTE8L///Y9ffvnFPIo9ISEBg8FAnz59cHJy4r///S+Ojo6Ehoby+++/c+zYMQYNGoSnpydLly7FaDQSERHRUF+5ziRRCyHETeRf//oXjz32GP369cPHx4eXX36ZwsLCRo/j5ZdfJjs7m0ceeQSNRsOECRMYNmxYrWaZGjlyJB9//DEzZ87kueeeo3Xr1syfP5/o6GjANB/0u+++y+TJkzEYDHTp0oX//e9/eHt74+HhwS+//ML06dMpKysjLCyMH374gU6dOjXQN647ldLYNw2s7OTJk4SEhJCZmUmLFi1u+HiVBiMateriX4H5maC2A7cbm39UCNF4ysrKOH78OK1bt8bBwcHa4dyUjEYjHTp04KGHHuKtt96ydjj14mq/V7XJRXKP+ga8tGgP3d9KJPVU1V+jy/8JszrDti+tG5gQQti49PR05s2bx6FDh0hJSeGpp57i+PHj/O1vf7N2aDZHEvUNOH+hgsKyStYeqnpEwb8TqDRw4ax1AxNCCBunVqtJSEigV69e9O/fn5SUFFauXEmHDh2sHZrNkXvUN2BwuC+J+3NYeyiPibeFQaeR0PFu0LlaOzQhhLBpISEhl43YFjWTRH0DBoebXly/KyOfgtIK3B3l0SwhhBD1S7q+b0CIlxNtfZ0xGBU2HjljudEKjzsIIYRofiRR36DB4X4ArE3LMxWc2gnzboNv7rZiVEIIIZoLSdQ3aHCEqft77aE80+vxHDxMyTpzK1SUWjc4IYQQTZ4k6hvUp7UXOjs12YVlpOUUgVcbcA0EQzmc3H7tAwghhBBXYdVEHR8fT69evXB1dcXPz4+RI0eSlpZ2zf0WLlxI+/btcXBwoEuXLrWeGq0+Odhr6NvWG6jq/lapTNNeApyQEY1CCCFujFUT9dq1a4mLi2PLli0kJiZSUVHB0KFDKSkpueI+mzZtYvTo0YwfP57du3czcuRIRo4cSWpqaiNGbql69PfaQ1X3qaunvTyxwUoRCSHE9YuOjmbSpEnm9VatWjFr1qyr7qNSqViyZMkNn7u+jnM106dPp2vXrg16joZk1US9fPlyxo0bR6dOnYiKiiIhIYGMjAx27tx5xX0+/vhjYmNjefHFF+nQoQNvvfUW3bt3Z/bs2Y0YuaXqRL39xDlK9JUXr6hPboeKMqvFJYRo3kaMGEFsbGyN29avX49KpWLv3r21Pu727duZMGHCjYZn4UrJMisri+HDh9fruZobm7pHXVBQAICXl9cV62zevJmYmBiLsmHDhrF58+Ya6+v1egoLC81LUVFR/QVcpbWPMy29nKgwKGw6eha824GLPxj0poFlQgjRAMaPH09iYiInT568bNv8+fPp2bMnkZGRtT6ur68vTk5O9RHiNQUEBKDT6RrlXE2VzSRqo9HIpEmT6N+/P507d75ivezsbPz9/S3K/P39yc7OrrF+fHw87u7u5uXSeUrri0qluqT7O9d0n1q6v4UQDeyuu+7C19eXhIQEi/Li4mIWLlzI+PHjOXv2LKNHjyY4OBgnJye6dOnCDz/8cNXj/rXr+/DhwwwaNAgHBwc6duxIYmLiZfu8/PLLhIeH4+TkRJs2bXjjjTeoqKgATNNNvvnmm+zZsweVyjSJUXXMf+36TklJ4bbbbsPR0RFvb28mTJhAcXGxefu4ceMYOXIkM2fOJDAwEG9vb+Li4sznuh5Go5EZM2bQokULdDodXbt2Zfny5ebt5eXlTJw4kcDAQBwcHAgNDSU+Ph4ARVGYPn06LVu2RKfTERQUxLPPPnvd564Lm0nUcXFxpKamsmDBgno97quvvkpBQYF52b9/f70ev1p1ol6TVvWYVquqRJ0uiVqIJq28pPaLofLi/oZKU9lfH9e80r61YGdnxyOPPEJCQgKXToS4cOFCDAYDo0ePpqysjB49evDHH3+QmprKhAkTePjhh9m2bdt1ncNoNHLfffeh1WrZunUrn3/+OS+//PJl9VxdXUlISGD//v18/PHHzJs3j48++giAUaNG8cILL9CpUyeysrLIyspi1KhRlx2jpKSEYcOG4enpyfbt21m4cCErV65k4sSJFvVWr17N0aNHWb16Nf/5z39ISEi47I+Vq/n444/58MMPmTlzJnv37mXYsGHcfffdHD58GIBPPvmE3377jZ9++om0tDS+++47WrVqBcDPP//MRx99xBdffMHhw4dZsmQJXbp0ue5z14VNvEJ04sSJ/P7776xbt+6a030FBASQk5NjUZaTk0NAQECN9XU6nUW3SkPNu9q3rTdajZqT50s5dqaEtqFV96kzt0OlHuyka0eIJumdoNrv82ACdLrX9Png/2DhOAgdAI/+cbHOrC41T+AzvaBWp3rsscf44IMPWLt2rXke5vnz53P//febexKnTJlirv/MM8+wYsUKfvrpJ3r37n3N469cuZKDBw+yYsUKgoJMbfHOO+9cdl/59ddfN39u1aoVU6ZMYcGCBbz00ks4Ojri4uKCnZ3dFf+tBvj+++8pKyvjm2++wdnZ9Erm2bNnM2LECN577z1zb6qnpyezZ89Go9HQvn177rzzTpKSknjiiSeuq81mzpzJyy+/zP/93/8B8N5777F69WpmzZrFnDlzyMjIICwsjAEDBqBSqQgNDTXvm5GRQUBAADExMdjb29OyZcvrascbYdUrakVRmDhxIosXL2bVqlW0bt36mvv07duXpKQki7LExET69u3bUGFeF2edHb1aewJVj2n5RoCTD1SWwqldVo1NCNF8tW/fnn79+vH1118DcOTIEdavX8/48eMBMBgMvPXWW3Tp0gUvLy9cXFxYsWIFGRkZ13X8AwcOEBISYk7SQI3/3v7444/079+fgIAAXFxceP3116/7HJeeKyoqypykAfr374/RaLR4dLdTp05oNBrzemBgILm5udd1jsLCQk6fPk3//v0tyvv378+BAwcAU/d6cnIyERERPPvss/z555/meg8++CClpaW0adOGJ554gsWLF1NZWUlDsuoVdVxcHN9//z2//vorrq6u5vvM7u7uODo6AvDII48QHBxsvj/w3HPPMXjwYD788EPuvPNOFixYwI4dO/jyS+vPAT043JeNR86y9lAejw1ober+3v+rqfs71Lp/SAgh6uifp2u/j+aSHrT2I0zHUP3lumhSyo3FdYnx48fzzDPPMGfOHObPn0/btm0ZPHgwAB988AEff/wxs2bNokuXLjg7OzNp0iTKy8vr7fybN29mzJgxvPnmmwwbNgx3d3cWLFjAhx9+WG/nuJS9vb3FukqlwliP8yt0796d48ePs2zZMlauXMlDDz1ETEwMixYtIiQkhLS0NFauXEliYiJPP/20uUfjr3HVF6teUc+dO5eCggKio6MJDAw0Lz/++KO5TkZGBllZWeb1fv368f333/Pll18SFRXFokWLWLJkyVUHoDWW6AjTe7+3HDtLWYXB1NUFpu5vIUTTpHWu/aK55BpIY2cqs3e8vuPWwUMPPYRareb777/nm2++4bHHHkOlUgGwceNG7rnnHv7+978TFRVFmzZtOHTo0HUfu0OHDmRmZlr8O7xlyxaLOps2bSI0NJTXXnuNnj17EhYWRnp6uuXX1WoxGAzXPNeePXss3qWxceNG1Go1ERER1x3z1bi5uREUFHTZFJsbN260GGzs5ubGqFGjmDdvHj/++CM///wz586dA8DR0ZERI0bwySefsGbNGjZv3kxKSv394fVXVr2ivnTww5WsWbPmsrIHH3yQBx98sAEiujFhfi4EujuQVVDGlmNnie54DwT3gMAoa4cmhGjGXFxcGDVqFK+++iqFhYWMGzfOvC0sLIxFixaxadMmPD09+de//kVOTs51PwETExNDeHg4Y8eO5YMPPqCwsJDXXnvNok5YWBgZGRksWLCAXr168ccff7B48WKLOq1ateL48eMkJyfTokULXF1dL3ssa8yYMUybNo2xY8cyffp08vLyeOaZZ3j44Ycve9rnRrz44otMmzaNtm3b0rVrV+bPn09ycjLfffcdAP/6178IDAykW7duqNVqFi5cSEBAAB4eHiQkJGAwGOjTpw9OTk7897//xdHR0eI+dn2zmVHfzYHlY1p54OoPLXpY/nUthBANYPz48Zw/f55hw4ZZ3E9+/fXX6d69O8OGDSM6OpqAgABGjhx53cdVq9UsXryY0tJSevfuzeOPP87bb79tUefuu+/m+eefZ+LEiXTt2pVNmzbxxhtvWNS5//77iY2N5dZbb8XX17fGR8ScnJxYsWIF586do1evXjzwwAMMGTKk3l9o9eyzzzJ58mReeOEFunTpwvLly/ntt98ICwsDTCPY33//fXr27EmvXr04ceIES5cuRa1W4+Hhwbx58+jfvz+RkZGsXLmS//3vf3h7e9drjJdSKddzWduMnDx5kpCQEDIzM685wrwulqVk8dR3u2jj68yqF6Lr/fhCiPpXVlbG8ePHad26NQ4ODtYORzQTV/u9qk0ukku9etY/zAeNWsWxvBIyz10gxHASNn8KKg2MmGXt8IQQQjQx0vVdz9wc7OnR0vSY1ppDeabXiO76BlIWWr4EQQghhLgOkqgbwOCIqvvUaXng1wkGTIYHvgZuqrsMQggh6oEk6gZQPaBs09Ez6I0KxEyD8GGgaZhn7IQQQjRfkqgbQMdAN3xcdFwoN7DzxHlrhyOEEKIJk0TdANRqFYPCfYCqx7SMBjiSBKveNn0WQtik+ny7lRD19fsko74bSHSEH7/sOsXaQ3m8GhsOCx8FfQG0vwOCulk7PCHEJbRaLWq1mtOnT+Pr64tWqzW/2UuI2lIUhfLycvLy8lCr1Wi12hs6niTqBjKwnQ8qFRzMLiKrqJzA0L5waDmc2CiJWggbo1arad26NVlZWZw+XYd3ewtRAycnJ1q2bIlafWOd15KoG4ins5aoFh4kZ+az7lAeo0L7VyXqDdBv4rUPIIRoVFqtlpYtW1JZWXnNd1ILcS0ajQY7O7t66ZmRRN2ABof7kpyZz9pDeYyKrppSLWOT6T61WnP1nYUQjU6lUmFvb99gsyAJURcymKwBRVc9T73+8Bkq/bqA1hXKCiBnn5UjE0II0VRIom5AkS088HCyp6iskt2niqHlLaYNJzZYNzAhhBBNhiTqBqRRqxgYdslbylpVdX+nb7zKXkIIIcRFkqgbWHTVW8rWHMqF0AGmwvSNIM9rCiGEuA6SqBvYwKoXn6SeKiTPtQPYO0Ppecjdb+XIhBBCNAWSqBuYn6sDnYLcAFh/LB9a9jFtkO5vIYQQ10ESdSOoHv299lAehFbdp5YBZUIIIa6DJOpGMDjcD4B1h/IwXHqfWpFpL4UQQlydvPCkEXRr6YGrzo7zFypIVdoQFTbM1AVeqQd7B2uHJ4QQwoZJom4E9ho1A8J8WJaazZojBUSN+cnaIQkhhGgipOu7kQy+9DEtIYQQ4jpJom4kg6oS9Z7MfM6XlENRDuxbIvephRBCXJUk6kYS5OFIuL8LRgU2HjoNH0fCwrFw9oi1QxNCCGHDrJqo161bx4gRIwgKCkKlUrFkyZKr1l+zZg0qleqyJTs7u3ECvkHREabR32uOFEBIHwiIhAvnrByVEEIIW2bVRF1SUkJUVBRz5syp1X5paWlkZWWZFz8/vwaKsH5V36deeygP45if4cn1F1+AIoQQQtTAqqO+hw8fzvDhw2u9n5+fHx4eHvUfUAPr2coTJ62GvCI9B3Iv0CnI3dohCSGEsHFN8h51165dCQwM5Pbbb2fjxqbzKk6dnYZ+bb2BqreUAVSUQvkFK0YlhBDCljWpRB0YGMjnn3/Ozz//zM8//0xISAjR0dHs2rXrivvo9XoKCwvNS1FRUSNGfDnzY1ppebD0JXi3JaQstGpMQgghbFeTeuFJREQEERER5vV+/fpx9OhRPvroI7799tsa94mPj+fNN99srBCvyfQ60X3sSj+PvrULOkO56XWiPcZaOzQhhBA2qEldUdekd+/eHDly5UecXn31VQoKCszL/v3WnV6ypbcTbXycqTQq7NF0MRWe2CDPUwshhKhRk0/UycnJBAYGXnG7TqfDzc3NvLi6ujZidDWrfvnJ7+dbgNoeCk/B+RPWDUoIIYRNsmqiLi4uJjk5meTkZACOHz9OcnIyGRkZgOlq+JFHHjHXnzVrFr/++itHjhwhNTWVSZMmsWrVKuLi4qwRfp0Nrpr2cuXhQpTg7qZCmfZSCCFEDax6j3rHjh3ceuut5vXJkycDMHbsWBISEsjKyjInbYDy8nJeeOEFTp06hZOTE5GRkaxcudLiGE1B3zbe6OzUnC4o43yn3nhlbjXdp+7+sLVDE0IIYWNUinJz3Rw9efIkISEhZGZm0qJFC6vF8cjX21h3KI+5t+QzPPlpcG8Jz6dYLR4hhBCNpza5qMnfo26qqh/TWpQbDCoNFGTA+XQrRyWEEMLWSKK2kupEvT69FENQN1NhetN5eYsQQojGUadEnZmZycmTJ83r27ZtY9KkSXz55Zf1Flhz19bXmRaejpQbjJx0q0rUJyRRCyGEsFSnRP23v/2N1atXA5Cdnc3tt9/Otm3beO2115gxY0a9BthcqVQq81X1On3VS1xOrLdiREIIIWxRnRJ1amoqvXv3BuCnn36ic+fObNq0ie+++46EhIT6jK9Zq07U32cHme5T56dDwclr7CWEEOJmUqdEXVFRgU6nA2DlypXcfffdALRv356srKz6i66Z69fOB3uNigPnQO/bBewcIC/N2mEJIYSwIXVK1J06deLzzz9n/fr1JCYmEhsbC8Dp06fx9vau1wCbMxedHT1DvQD4X0Q8vJIB7YZYOSohhBC2pE6J+r333uOLL74gOjqa0aNHExUVBcBvv/1m7hIX16f6LWV/ZNiBnc7K0QghhLA1dXozWXR0NGfOnKGwsBBPT09z+YQJE3Bycqq34G4G0RG+vLvsIJuPnaWswoCDvcY0QYdKZe3QhBBC2IA6XVGXlpai1+vNSTo9PZ1Zs2aRlpaGn59fvQbY3EX4u+LvpqOswsjppe/DnFsg9WdrhyWEEMJG1ClR33PPPXzzzTcA5Ofn06dPHz788ENGjhzJ3Llz6zXA5u7Sx7RyTqdD3gGZoEMIIYRZnRL1rl27GDhwIACLFi3C39+f9PR0vvnmGz755JN6DfBmEB1h6oX4uvgWeOhbuO0NK0ckhBDCVtQpUV+4cME8r/Off/7Jfffdh1qt5pZbbiE9Xd5XXVv92/mgUatIPOvLycAYcJaR80IIIUzqlKjbtWvHkiVLyMzMZMWKFQwdOhSA3Nxc3Nzc6jXAm4G7oz3dQjwAWHfojHWDEUIIYVPqlKinTp3KlClTaNWqFb1796Zv376A6eq6W7du9RrgzaL6PvX+lJ2w5l3Y+oWVIxJCCGEL6pSoH3jgATIyMtixYwcrVqwwlw8ZMoSPPvqo3oK7mVTfpy7OTIE18bDjaytHJIQQwhbU6TlqgICAAAICAsyzaLVo0UJednIDOgW54e2sZW1JGDgAeQeh5Aw4+1g7NCGEEFZUpytqo9HIjBkzcHd3JzQ0lNDQUDw8PHjrrbcwGo31HeNNQa1WMSjcl/O4kevY1lQo81MLIcRNr06J+rXXXmP27Nm8++677N69m927d/POO+/w6aef8sYb8mhRXUVXvU50i7GDqUCepxZCiJtenbq+//Of//DVV1+ZZ80CiIyMJDg4mKeffpq333673gK8mQxo54NKBcuK2nK3FjghV9RCCHGzq9MV9blz52jfvv1l5e3bt+fcuXM3HNTNyttFR2SwO9uMVW2buw8uSHsKIcTNrE6JOioqitmzZ19WPnv2bCIjI284qJvZ4Ag/zuJOljbUVJC+yboBCSGEsKo6dX2///773HnnnaxcudL8DPXmzZvJzMxk6dKl9RrgzWZwuC+fJB1mXXkEo0g33afucJe1wxJCCGEldbqiHjx4MIcOHeLee+8lPz+f/Px87rvvPvbt28e3335b3zHeVKJauOPuaM/68ghTQboMKBNCiJtZnZ+jDgoKumzQ2J49e/j3v//Nl19+ecOB3azsNGoGhPmwdW/VyO/sVCg9D46eV99RCCFEs1SnK2rRsKLDfcnDg5OaFoAC6ZutHZIQQggrsWqiXrduHSNGjCAoKAiVSsWSJUuuuc+aNWvo3r07Op2Odu3akZCQ0OBxNrbq936vKw83FciLT4QQ4qZl1URdUlJCVFQUc+bMua76x48f58477+TWW28lOTmZSZMm8fjjj1u8b7w58HNzoEOgG/8z9OVARBx0vt/aIQkhhLCSWt2jvu+++666PT8/v1YnHz58OMOHD7/u+p9//jmtW7fmww8/BKBDhw5s2LCBjz76iGHDhtXq3LYuOsKXuVmd+FIdzEfBXa0djhBCCCup1RW1u7v7VZfQ0FAeeeSRhoqVzZs3ExMTY1E2bNgwNm9ufvdwzd3fh/IwGhUrRyOEEMJaanVFPX/+/IaK47pkZ2fj7+9vUebv709hYSGlpaU4Ojpeto9er0ev15vXi4qKGjzO+tAj1BMXnR0VJefI3PQToT6u0P4Oa4clhBCikTX7Ud/x8fEWV/0dO3a0dkjXxV6jpn87b25TJxO6cgKsn2ntkIQQQlhBk0rUAQEB5OTkWJTl5OTg5uZW49U0wKuvvkpBQYF52b9/f2OEWi8Gh/ux1diBTE0IBPcERbrAhRDiZtOkEnXfvn1JSkqyKEtMTDS/xrQmOp0ONzc38+Lq6trQYdabwRG+ZOHN4AvvURD9NqhU1g5JCCFEI7Nqoi4uLiY5OZnk5GTA9PhVcnIyGRkZgOlq+NLBaU8++STHjh3jpZde4uDBg3z22Wf89NNPPP/889YIv8EFezgS5ueCUYENR85YOxwhhBBWYNVEvWPHDrp160a3bt0AmDx5Mt26dWPq1KkAZGVlmZM2QOvWrfnjjz9ITEwkKiqKDz/8kK+++qrZPZp1qerR3xsOnoLsFCtHI4QQorGpFOXmuvF58uRJQkJCyMzMpEWLFtYO55rWH85j0r8T2ejwHDq1EdUrGaB1tnZYQgghbkBtclGTukd9M+rVyosL9l6cUdxQGSshc6u1QxJCCNGIJFHbOAd7DX3berPV2N5UcEKmvRRCiJuJJOomYHC4L1uMVc9/n5AJOoQQ4mYiiboJGBzuy1ajaX5q5dROKL9g5YiEEEI0FknUTUArH2fUnq3IUrxQGSvg5HZrhySEEKKRSKJuIgZH+LGl6qpa7lMLIcTNQxJ1EzE44pLu73RJ1EIIcbOQRN1E3NLGm10q04Ay5eROqCizckRCCCEagyTqJsJJa4d/q07kKB6oDXo4tcPaIQkhhGgEkqibkMERfubub7lPLYQQNwdJ1E1I9CX3qQ3HJVELIcTNQBJ1E9LW14Vjzt0oVzQU6I0yP7UQQtwEJFE3ISqVilYRXYnUf8UnQR/I/NRCCHETkETdxAyO8KMMHesO5Vk7FCGEEI1AEnUT07+dN3ZqFcfOlJCZfcba4QghhGhgkqibGFcHe6JbqPhd+08C5nWBynJrhySEEKIBSaJugrp3aEeg6iz2hguQk2rtcIQQQjQgSdRNUHSEP/8of57Bxrno/aOsHY4QQogGJIm6CeoQ6Eq6SxTp5e7sOHHe2uEIIYRoQHbWDkDUnkqlYnC4L4t2nkS/5kPYkAL+nSGgMwR0Ad/2YKezdphCCCHqgSTqJio6wpSoHbO2gmEnnFh/caPaDnzCLyZv/6oE7uJnvYCFEELUiSTqJmpAOx/s1CqmXXiISHVPOqoz6KE7RZhyAidDIeTuNy0pP13cydnPlLgj/w+iRlkveCGEENdNEnUT5eGk5ZPR3Viy24+1me1YVKSHCgCFAM7RUZ1ON+1JejudJsx4As+yTFQluXB0FbTsd/FA59Phx79DcA8YMctK30YIIcSVSKJuwu7oEsgdXQJRFIXTBWUkZ+SzO+M8yZlebDzly6qy7lA1bbUjZUSoTjLANQvjiTb425+gW0sPOhTsxT57L/CX94Z/X3XFbe4+7wJerUGtadTvKIQQNztJ1M2ASqUi2MORYA9H7owMBKDCYORgVhHJmefZnZlPckY+yWccSC5sB4XAgX0A+Ntd4D7v12nt7IzjntN0a+lBsKsdqqOrwFAOh5ZfPJG9E/h1tLzv7dseHD0a/DsqikKRvpKzxeWcLdZzpric0opKerXyooWnU4OfXwghrEWlKDfXFEwnT54kJCSEzMxMWrRoYe1wGlX+hXKSM/PNy+6MfApKKy6r5+9sx33+p7nFKYv2HMen5DCavINQWVrzgV38TYPXYt6EFj1MZYYK06C2q0wcoq80cK6knLPF5Zwp1puScIm+at302VxeXE65wVjjcaJauBPbOZDhnQNo5eNc63YRQojGVptcZBOJes6cOXzwwQdkZ2cTFRXFp59+Su/evWusm5CQwKOPPmpRptPpKCsru65z3cyJ+q8UReHE2QtV3eWm5L3/dCGVRstfCZUK2vs6EeNfzC3OWbRXpeNVdAhVTioUnTbXMz6+mgLPzpwt0aPZPo+Q3R+QFnw/K1o8y9liPWeL9GgLjrK/zJucEgNFZZW1jtlFZ4e3ixZvZy0KkJyZbzHbZ4dAN+7oHMDwLgG083Ota9MIIUSDqk0usnrX948//sjkyZP5/PPP6dOnD7NmzWLYsGGkpaXh51fz40Rubm6kpaWZ11Uy3WOdqFQqWvs409rHmfu6m35RyioM7DtdwO6MfHOX+an8Ug7kXuBArppPCQaCcdYOpEsLd1xdS3EsPIZn6Ql+/uwExcYsAGbYbeIRuwusO5rPJ2mHAfDlPNsd4qhQNKQr/hy1D+IYweRoW3LeqTUX3Nrg4uaJt7MWbxcd3i5afFy0eDvr8HHV4e2sxcHe8h55XpGeP/dnsywlm83HznIgq5ADWYV8mHiIMD8XhncOYHiXQNoHuMrviRCiSbL6FXWfPn3o1asXs2fPBsBoNBISEsIzzzzDK6+8cln9hIQEJk2aRH5+fp3OJ1fUtZdbVDVQrSpx7z2ZT0m54Yr13R3t8XdW0cnhHI7Ormg8W+LtoiXccJih2x7HznDhyidzDQLfcFNXevUS0gfsHa4Z5/mSchL357A0NYuNR85QYbj4q93K24nhXUzd412C3SVpCyGsqsl0fZeXl+Pk5MSiRYsYOXKkuXzs2LHk5+fz66+/XrZPQkICjz/+OMHBwRiNRrp3784777xDp06dajyHXq9Hr9eb10+dOkXHjh0lUd8Ag1HhcG4RKScLsNOo8HauvvrV4emkRWt3lTfTGo2m7vK8NDhzGM5U/cxLg5LcmveZcvjiy1pSf4H8DAi7Hfxr/m8OUFBaQdKBHJalZrP2UB7llRfvbwd7OJqvtLuFeKBWS9IWQjSuJtP1febMGQwGA/7+/hbl/v7+HDx4sMZ9IiIi+Prrr4mMjKSgoICZM2fSr18/9u3bV+OXjY+P580332yQ+G9WGrWK9gFutA9wq/3OajW4tzAt7YZYbis9X5W8D1Ul8kNQlA3Ovhfr7P3RNBJd63wxUZ87Drv/a3oWPLgHuPrj7mjPfd1bcF/3FhTrK1l9MJdlqVmsPpjHqfxSvtpwnK82HCfAzYHYzgHEdg6gVysvNJK0hRA2xqpX1KdPnyY4OJhNmzbRt29fc/lLL73E2rVr2bp16zWPUVFRQYcOHRg9ejRvvfXWZdvlirqZ2TYPMrZA36dNSRlg17fw28SLddxDILj7xcQd2BV0LgCUlhtYeyiXZanZJB3IpVh/cUCbj4uWoZ0CuKNzIH3aeGGvkTlrhBANo8lcUfv4+KDRaMjJybEoz8nJISAg4LqOYW9vT7du3Thy5EiN23U6HTrdxQkqCgsL6x6wsL7eT5iWS3m3hW5/h1O7IPcAFGSalv1Vt05UavDtAMHdcQzuQWxwD2If7EKZUcXGI2dYmpJN4v5szhSX8/3WDL7fmoGHkz1DO/ozvHMg/dv5XL07XwghGpBVE7VWq6VHjx4kJSWZ71EbjUaSkpKYOHHi1XeuYjAYSElJ4Y477mjASIVNC+1nWgD0RZC1B07ugFM7Tcm78CTk7jMtu7811QvqhsOENQzp4M+QDv6U5/uxOUfD8n3ZrNiXw7mScn7acZKfdpzE1cGOmA7+DO8cwKBw38tGngshREOy+uNZkydPZuzYsfTs2ZPevXsza9YsSkpKzM9KP/LIIwQHBxMfHw/AjBkzuOWWW2jXrh35+fl88MEHpKen8/jjj1vzawhboXOFVgNMS7Wi7KqkvfNi8r50IJqhAu3srgzWujD4yQ28dU9nth0/x4qUkyzdf4a8Ij2Ld59i8e5TOGk13Nbej+GdAxnQzgcnnQY7tUpGkQshGozVE/WoUaPIy8tj6tSpZGdn07VrV5YvX24eYJaRkYFafbHb8fz58zzxxBNkZ2fj6elJjx492LRpEx07drTWVxC2zjUA2t9pWsA08ryi5OL2c8fBaABjBbj4Y6dW06+dD/2SX2K6azLnQjqzraI1P2f7s74okN/3ZvH73iyLU2g1auw1Kuzt1Nhr1BfXNaZ1ezs12kvXNWq0dirs1Bc/W2yrrmv3l/VLjuXrqiPc3xVXB/tGbEwhRGOz+nPUjU2eoxY1qigzPfblG36xbFYk5KdbVDOq7clxbMdmfSjbSluQrXiSq3iSo3hyDlcUGv9edrCHIxEBrqbF3/Szja8zOjvpohfCVjWZ56itQRK1uG4XzsHp3aau8lM7TPe9L5y5YnVFbUfegLc40/7vVBiMqPIz8DjyC8UurTgdPJwKg5Fyg5GKSiMVRoUKg5EKQ9XPSmPV9uryqvXKv6wbFCoqTcc5db6U7MKaX51rpza9dS48wJX2/q6mnwGuhHg6yXPjQtiAJjPqWwib5uRleta7+nlvRTGNJj+105S089KgOBuKcqAkD5WxEj9fP/yCqp4vL9kEez6CoO50vH3cxePO7gXlF0xd8pcuXoHgUr0eaDr/Ne59518o51BOMWnZhaTlFJGWXcTB7CKKyio5nFvM4dxi/uBiN72jvYZwfxciAlwJ93elfYAb4QEu+Lro5D67EDZKErUQ10ulAo+WpqXTvZbbDBVQnAsOl7wExtUfuj1sql9NUSA/0zQTWeHJq59PbX8xiQ98ASKGm8pLzsDpZPAIwcM3gt6tvejd2uuSUyhkF5aRll10cckp4nBuMaUVBvacLGDPyQKLU3k5awn3dzEl7qru84gAV1x08k+EENYm/xcKUR809uAebFlW/cKVv3pmh2kkelE2FGVBcY7pZ1HV1XlRlqmL3Vhx8Znwikvej56xBX4cAy16w+OJF8vn3QaVelROXgQ6ehHo5EW0kze09IL2XhgcPMmqcOFIkZZ9+fak5Bk5lFvMibMlnCspZ8uxc2w5ds7yK3g40j7gYtd5uL8rbX1d6vxcuaIoGIwKlca//jSafhpqLjf8pb6DvUZe/ypuGpKohWhMKtXFV6heTWW56d3n1Qk9uPvFbWoN+HUC73aW++Tsv/Kc4YAGaFG1RINpvvC7ZlHW5W8cyS3m9OHd+O37iv0VgXxyIZbswjJO5ZfiXnCA42laFiguFOCCWq0h1NsJB3tNjUnUnHSNCgaDZbmxHkfEtPV15h+D2zKya7C8kEY0azKYTIjmQFFMA99Kz8GF83DhbNXnc3/5fM70ufoK/YGvofP9ps/7f4OfHjbNVjb+T/IvlJOWXUTnn/riXGZ6e6ARFYWKE+cVF0pxQI89ZYrW9BPTT71iz2LjADYbTc+qB3KWuzSbyVU8+NV48fn2XqqD2KkM6BV79GipVFcvDlSqtBjUOoxqezQaNRq1Cju1quqnmtP5pRRVvf410N2Bxwe24f96heAsXfWiiZDBZELcbFQqy6vua6koNSVtB/eLZT7hcOvr5pnKPJy09GnjDa5eoJSBvgA1Ch6qEjxUJVc4sMmgQcMp7jwYO7UKp5Pr8VvyPZU+HZg6bjp2ajUajQqnL6ehPnv4ygcxAkYV4AAqHagdof9zcMtTFJVVsGDzUQ5u+IWVBW156/cyPl11mLF9WzGuXys8nbXX3xZC2DhJ1ELcjOwdL7+n7tfetPxV3BbTT0OFaYYz81V5KVSWVS36qnU9VJYR0K4/+JkmQqGyBUSOws41EG+Xi+/dx6uNqRv/kv3Mi5li6s6vLIWyfPM2Vwd7nggrhrXvoXf1YJj9fE6cK+XjpMN8u24/9/QO44mBbQjycKy3JhPCWiRRCyGuj8bedLVdPTf49QroDPd9eXn5mJ9qrq8oYCj/SwLXm5K1yyVT4pYVgE8EOp8wkh66leWp2Xy2+jBfnHuUsu1a1m7rgLFlP/oNGUHrNhG1i1kIGyL3qIUQTZuhwvRHBKAUnEL10eWvE86zC0Tduj/eHW+DVv3BI/Saz6gL0ZDkHrUQ4uahufiuc5V7MLx0HDK2kJuSxIUj6wkpO4RvZRYcXmRaAMUtGFVof9Osa60GmEbQS+IWNkoStRCieXHygvZ34NfeNPXt0ZOnWf3n/6g8voFeqgNEqo5hX3gKUn4yLQDP77v4yFzpedC5g1oe+RK2QRK1EKJZa9siiLaP/YPT+Y/w1frjPL7tMB0MB+mjPshgbRqtHUtxcA7EPMztl3/AyW1w92zocJc1Q7cJxfpKjuYWcyS3mCN5xWSeu4CdWoWDveaSRY3jJZ8v3eZ4SZmjvQbdJZ/tNfLH0PWQRC2EuCkEeTgydURHnrmtHf/Z3IH5m07w0YUKVBeM+L63mvEDWvO33iG4ZqeYrqovHRWf+gvs+cHUVR46AIK6WnS5N3WKonCmuNycjKsT89G8YrIKap74pT5o1Coc7NQ4ajXo7KoSvlaDg53lHwGXJnwvZx3923nTOcj9pnkznQwmE0LclEr0lSzYnslX64+Zk5Grgx3j+gQzvm0BHm37gKbqWubXONj934s7q+1Nj5f5hJmePzcv7SyfTbcxRqPCyfOlHMkrMiXi3BKO5JmSckFpxRX383HR0c7PmXZ+LrTydgagtNxAWaWBsgojpRUGyioM6C/5XFZhoLTCiN782VS3rNJAfWQdb2ctg8J9iY7wZWCYL15N7Nl5mebyKiRRCyEuVV5p5NfkU3y+9ihH80wvctHZqXmoZwgTBrUhxMsJcg/A0dWQvtG0lJ6/8gFdAkwJvP2dcMtTF8sVpdEGrOkrDRw/U2JKxFVXyUdyizmWV4y+0ljjPioVtPB0pJ2vC+38Lll8XXF3qr/eA0VR0Fca0VclbYuEX/VZf2liv+SzvsL0vTYdPUtx1ZvpqmOPauFBdIQvg8N9iWzhgcbGr7YlUV+FJGohRE2MRoXEAzl8tuYoezLzAVPX7IjIQJ6Mbkv7ALfqilB02jTN6ZnDcOZQ1XLYNO1ptZ6PwV0fmT6Xl8DMcNNV+GMrQOtkKi/KMV2B2zvUKebCsgqL+8fVnzPOXbjie9W1GjWtfUxXx23NydiFNr7OONhr6hRHYyuvNLIz/TxrDuWyNi2Pg9lFFts9nezNV9uDwnwtX7RjIyRRX4UkaiHE1SiKwuZjZ5m75ijrD58xl9/W3o+notvSq5XXlXcuK4AzR0yJ26s1tLzFVJ61B74YBE4+8NJRKgymLmLdglFoT6yiwi2EUre2lLi2odClNeedWnFGF0qByo2yStOVZmnVlWVpuYGMcxc4kltMbpH+iqG46uwuJmI/F9pWXSmHeDpi18wGcWUXlLH2UC5r0vLYcPiM+T3wYLra7hLsTnS4L4Mj/OgaYhtX25Kor0IStRDieqWeKmDu2qMsTcky31ftGerJXZGBVBpNXbiXJtGyvyTU6m7bivIKvCpO41Jxjo0V4VRWXe7+oX2VTur0K57/vOLCUSWIo8YgjihBHFWCSDG2Jg9PAHSUE+5SSoi3G96BrcxJOcI+B2+dEZViBMVo6gVQjKAYwGi4+PnSbd5tTQtAaT4cTQKNznLke9oy0zSsxqrjGCsvWa6wHtoPOo007X/hHCydAqjggX9fPO6qtyFj81WOWXFxXaM1Pfcedjv0+cdlbVZhMLIr/TxrD+WxJi2P/VmFFtvdHe0ZGOZDdIQfg8N98XW1ztW2JOqrkEQthKit42dK+HLdUX7eeYpyQ833eOtCpVJoYV9Me7tswtRZtFWfppVyihDjSXwMuai5/J/njaFxnOrylCkhF+/A+acHwL8zPLXxYqVPusO5o7UL5tbXYfCLps/ZKfD5ANMrW6cculjn30Mhc2vtjtv7H3DH+6bPRdnwYQSoNDDtkrnPF4yBg7/X7rhdx8DIz0yfK8vh4yjTHxr/9z04VN2mKC8ht1TNmsNnWHsoj/WH8igsq7Q4TOdgN6LD/Rgc4Uu3EI9G622QN5MJIUQ9au3jTPx9kUyKCec/m05wOLcYx6pHhhy1F58XdtSqLZ4fvnz7xXKdvRqdnRrVlQaYlV8wJdvq+99V98L79xsEESGmOscdwM7h8kfFnH2gvBhUalNSVKlNL3CxWNdUfVaZPl/6DnetC7QaCI4elscN7WfqvldrTPOZa+xNP6vXzcsl6y16Xdxf5wax75rKL9U3Djrfd4Vj2FuWlRdX3Vpoc3H/c8dM4wb0RaBzvVi++B/4HVvLQz7hPOQbgWFIGMcIZu1ZL37LsGPv6RJSTxWSeqqQ2auP4OZgx8AwXwZH+BId7oufW93GDtQ3uaIWQgjRtFXqIScVivMgIvZi+ZxbIO9AzftodFR6tSXLPpQUvT9rznmyp8yf40og5Zj+8OkQ6EZ0VdLuHupZry9oka7vq5BELYQQN4lKPZw9CmfSIO9Q1c+q0fqGmgfibWnxGPFl97P3VAHuShG3qXeTpoSQoQ1jQJgPg8N9uSsqCBfdjXVIS9e3EEIIYacD/46m5VJGA+SnX5K8D0HeQThziFv69OfXLgM4W6zn4IZf6L/lc44RzG1lH7AsNZsV+7IZ1ikAGnEMmiRqIYQQNxe1xnSP26uNZVe5ophGwAPeLjr6hwdB9kBaebVlSbf+rEnLJaewDM9GfguaTTxMN2fOHFq1aoWDgwN9+vRh27ZtV62/cOFC2rdvj4ODA126dGHp0qWNFKkQQohmq3pgXbU2g2Hc76jv/piuIR5Migkn/r7IRg/L6on6xx9/ZPLkyUybNo1du3YRFRXFsGHDyM3NrbH+pk2bGD16NOPHj2f37t2MHDmSkSNHkpqa2siRCyGEEA3P6oPJ+vTpQ69evZg9ezYARqORkJAQnnnmGV555ZXL6o8aNYqSkhJ+//3iM3e33HILXbt25fPPP7/m+WQwmRBCCGurTS6y6hV1eXk5O3fuJCYmxlymVquJiYlh8+bNNe6zefNmi/oAw4YNu2J9IYQQoimz6mCyM2fOYDAY8Pf3tyj39/fn4MGDNe6TnZ1dY/3s7Owa6+v1evT6i8Pwi4qKaqwnhBBC2CKr36NuaPHx8bi7u5uXjh07XnsnIYQQwkZYNVH7+Pig0WjIycmxKM/JySEgIKDGfQICAmpV/9VXX6WgoMC87N+/v36CF0IIIRqBVbu+tVotPXr0ICkpiZEjRwKmwWRJSUlMnDixxn369u1LUlISkyZNMpclJibSt2/fGuvrdDp0uotPpufn5wOQlZVVL99BCCGEqK3qHGQ0XsckL4qVLViwQNHpdEpCQoKyf/9+ZcKECYqHh4eSnZ2tKIqiPPzww8orr7xirr9x40bFzs5OmTlzpnLgwAFl2rRpir29vZKSknJd59u2bZsCyCKLLLLIIovVl23btl0zb1n9zWSjRo0iLy+PqVOnkp2dTdeuXVm+fLl5wFhGRgZq9cUe+n79+vH999/z+uuv889//pOwsDCWLFlC586dr+t83bp1Y9u2bfj7+1scty6Kioro2LEj+/fvx9XV9do73OSkvWpP2qx2pL1qR9qrduqzvYxGIzk5OXTr1u2ada3+HHVTVlhYiLu7OwUFBbi5uVk7HJsn7VV70ma1I+1VO9JetWOt9mr2o76FEEKIpkwStRBCCGHDJFHfAJ1Ox7Rp0yxGlYsrk/aqPWmz2pH2qh1pr9qxVnvJPWohhBDChskVtRBCCGHDJFELIYQQNkwStRBCCGHDJFHfgDlz5tCqVSscHBzo06cP27Zts3ZINmvdunWMGDGCoKAgVCoVS5YssXZINis+Pp5evXrh6uqKn58fI0eOJC0tzdph2ay5c+cSGRmJm5sbbm5u9O3bl2XLllk7rCbj3XffRaVSWbyWWViaPn06KpXKYmnfvn2jnV8SdR39+OOPTJ48mWnTprFr1y6ioqIYNmwYubm51g7NJpWUlBAVFcWcOXOsHYrNW7t2LXFxcWzZsoXExEQqKioYOnQoJSUl1g7NJrVo0YJ3332XnTt3smPHDm677Tbuuece9u3bZ+3QbN727dv54osviIyMtHYoNq9Tp05kZWWZlw0bNjTeyWv/dm6hKIrSu3dvJS4uzrxuMBiUoKAgJT4+3opRNQ2AsnjxYmuH0WTk5uYqgLJ27Vprh9JkeHp6Kl999ZW1w7BpRUVFSlhYmJKYmKgMHjxYee6556wdks2aNm2aEhUVZbXzyxV1HZSXl7Nz505iYmLMZWq1mpiYGDZv3mzFyERzVFBQAICXl5eVI7F9BoOBBQsWUFJScsUZ9YRJXFwcd955p8W/Y+LKDh8+TFBQEG3atGHMmDFkZGQ02rmtPilHU3TmzBkMBoN54pBq/v7+HDx40EpRiebIaDQyadIk+vfvf90Tz9yMUlJS6Nu3L2VlZbi4uLB48WI6duxo7bBs1oIFC9i1axfbt2+3dihNQp8+fUhISCAiIoKsrCzefPNNBg4cSGpqaqNMZiKJWggbFhcXR2pqauPeD2uCIiIiSE5OpqCggEWLFjF27FjWrl0ryboGmZmZPPfccyQmJuLg4GDtcJqE4cOHmz9HRkbSp08fQkND+emnnxg/fnyDn18SdR34+Pig0WjIycmxKM/JySEgIMBKUYnmZuLEifz++++sW7eOFi1aWDscm6bVamnXrh0APXr0YPv27Xz88cd88cUXVo7M9uzcuZPc3Fy6d+9uLjMYDKxbt47Zs2ej1+vRaDRWjND2eXh4EB4ezpEjRxrlfHKPug60Wi09evQgKSnJXGY0GklKSpL7YuKGKYrCxIkTWbx4MatWraJ169bWDqnJMRqN6PV6a4dhk4YMGUJKSgrJycnmpWfPnowZM4bk5GRJ0tehuLiYo0ePEhgY2CjnkyvqOpo8eTJjx46lZ8+e9O7dm1mzZlFSUsKjjz5q7dBsUnFxscVfn8ePHyc5ORkvLy9atmxpxchsT1xcHN9//z2//vorrq6uZGdnA+Du7o6jo6OVo7M9r776KsOHD6dly5YUFRXx/fffs2bNGlasWGHt0GySq6vrZeMdnJ2d8fb2lnEQVzBlyhRGjBhBaGgop0+fZtq0aWg0GkaPHt0o55dEXUejRo0iLy+PqVOnkp2dTdeuXVm+fPllA8yEyY4dO7j11lvN65MnTwZg7NixJCQkWCkq2zR37lwAoqOjLcrnz5/PuHHjGj8gG5ebm8sjjzxCVlYW7u7uREZGsmLFCm6//XZrhyaaiZMnTzJ69GjOnj2Lr68vAwYMYMuWLfj6+jbK+WX2LCGEEMKGyT1qIYQQwoZJohZCCCFsmCRqIYQQwoZJohZCCCFsmCRqIYQQwoZJohZCCCFsmCRqIYQQwoZJohZCCCFsmCRqIUSDUalULFmyxNphCNGkSaIWopkaN24cKpXqsiU2NtbaoQkhakHe9S1EMxYbG8v8+fMtynQ6nZWiEULUhVxRC9GM6XQ6AgICLBZPT0/A1C09d+5chg8fjqOjI23atGHRokUW+6ekpHDbbbfh6OiIt7c3EyZMoLi42KLO119/TadOndDpdAQGBjJx4kSL7WfOnOHee+/FycmJsLAwfvvtN/O28+fPM2bMGHx9fXF0dCQsLOyyPyyEuNlJohbiJvbGG29w//33s2fPHsaMGcP//d//ceDAAQBKSkoYNmwYnp6ebN++nYULF7Jy5UqLRDx37lzi4uKYMGECKSkp/Pbbb7Rr187iHG+++SYPPfQQe/fu5Y477mDMmDGcO3fOfP79+/ezbNkyDhw4wNy5c/Hx8Wm8BhCiKVCEEM3S2LFjFY1Gozg7O1ssb7/9tqIoigIoTz75pMU+ffr0UZ566ilFURTlyy+/VDw9PZXi4mLz9j/++ENRq9VKdna2oiiKEhQUpLz22mtXjAFQXn/9dfN6cXGxAijLli1TFEVRRowYoTz66KP184WFaKbkHrUQzditt95qnt+6mpeXl/lz3759Lbb17duX5ORkAA4cOEBUVBTOzs7m7f3798doNJKWloZKpeL06dMMGTLkqjFERkaaPzs7O+Pm5kZubi4ATz31FPfffz+7du1i6NChjBw5kn79+tXpuwrRXEmiFqIZc3Z2vqwrur44OjpeVz17e3uLdZVKhdFoBGD48OGkp6ezdOlSEhMTGTJkCHFxccycObPe4xWiqZJ71ELcxLZs2XLZeocOHQDo0KEDe/bsoaSkxLx948aNqNVqIiIicHV1pVWrViQlJd1QDL6+vowdO5b//ve/zJo1iy+//PKGjidEcyNX1EI0Y3q9nuzsbIsyOzs784CthQsX0rNnTwYMGMB3333Htm3b+Pe//w3AmDFjmDZtGmPHjmX69Onk5eXxzDPP8PDDD+Pv7w/A9OnTefLJJ/Hz82P48OEUFRWxceNGnnnmmeuKb+rUqfTo0YNOnTqh1+v5/fffzX8oCCFMJFEL0YwtX76cwMBAi7KIiAgOHjwImEZkL1iwgKeffprAwEB++OEHOnbsCICTkxMrVqzgueeeo1evXjg5OXH//ffzr3/9y3yssWPHUlZWxkcffcSUKVPw8fHhgQceuO74tFotr776KidOnMDR0ZGBAweyYMGCevjmQjQfKkVRFGsHIYRofCqVisWLFzNy5EhrhyKEuAq5Ry2EEELYMEnUQgghhA2Te9RC3KTkrpcQTYNcUQshhBA2TBK1EEIIYcMkUQshhBA2TBK1EEIIYcMkUQshhBA2TBK1EEIIYcMkUQshhBA2TBK1EEIIYcMkUQshhBA27P8DxVz3HIDs6k4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor        = torch.linspace(0, num_epochs,    len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75a15f-2b34-4653-acda-c2efe83ae26e",
   "metadata": {},
   "source": [
    "- conclusion: the model is learning well from the training data, and there is little indication of overfitting (no noticeable gap\n",
    "between the training and validation set losses).\n",
    "- Using the same plot_values function, let's now also plot the classification accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7a2b9a0e-7db1-41cd-b27c-35533aa26993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5fd9e-731d-41c3-97b6-befc54c20d5f",
   "metadata": {},
   "source": [
    "- conclusion: the model achieves a relatively high training and validation accuracy after epochs 4 and 5.\n",
    "- However, we previously set eval_iter=5 when using __train_classifier_simple__, so our estimations of training and validation\n",
    "performance were based on only 5 batches.\n",
    "- Next: calculate performance metrics for the training, validation, and test sets across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "26f72d49-b5f5-4cad-904f-b5f5dc7217df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.21%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c007d-0b03-4b11-9b80-4da553838cad",
   "metadata": {},
   "source": [
    "- conclusion: training and test set performances are almost identical. A slight discrepancy between the training and test set accuracies suggests minimal overfitting. Typically, the validation set accuracy is somewhat higher than the test set\n",
    "accuracy because the model development often involves tuning hyperparameters to perform well on the validation set, which might not generalize as effectively to the test set.\n",
    "- This situation is common, but the gap could possibly be minimized by adjusting the model's settings, such as increasing the dropout rate (drop_rate) or the weight_decay parameter in the optimizer configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c062e16-ebb0-4241-a4e0-a0c28f1957b1",
   "metadata": {},
   "source": [
    "## 6.8 Using the LLM as a spam classifier\n",
    "- Use the finetuned GPT-based spam classification model. __classify_review__ follows data preprocessing steps similar to those we used in the SpamDataset. After converting text into token IDs, the function uses the model to predict an integer class label & returns the corresponding class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "991b827f-d796-4dfa-bd3c-29ea3ebc5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    input_ids                = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "    input_ids                = input_ids[:min(max_length, supported_context_length)] #B\n",
    "    input_ids               += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor             = torch.tensor(input_ids, device=device).unsqueeze(0) #D\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "    \n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "67c09418-e590-4414-9210-6d273fb61310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "\"You are a winner you have been specially\"\n",
    "\" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "aa571bd1-cb79-47e7-a7e9-6351411427cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "\"Hey, just wanted to check if we're still on\"\n",
    "\" for dinner tonight? Let me know!\"\n",
    ")\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3322a5be-ce30-473b-9845-516880956f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later\n",
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c5530230-eb5d-497f-b322-6814ae4fcfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then reload\n",
    "model_state_dict = torch.load(\"review_classifier.pth\")\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c8d56-85c5-41a9-bfb8-78203899edee",
   "metadata": {},
   "source": [
    "# 7 Finetuning to Follow Instructions\n",
    "- TODO\n",
    "# Appendix A. Introduction to PyTorch\n",
    "- TODO\n",
    "# Appendix B. References and Further Reading\n",
    "- TODO\n",
    "# Appendix C. Exercise Solutions\n",
    "- TODO\n",
    "# Appendix D. Adding Bells and Whistles to the Training Loop\n",
    "- TODO\n",
    "# Appendix E. Parameter-ecient Finetuning with LoRA\n",
    "- TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
